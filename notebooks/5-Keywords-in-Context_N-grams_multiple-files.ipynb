{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cb4844f-df55-4404-a952-9f0d4ee1ddf3",
   "metadata": {},
   "source": [
    "# Keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2611d0c4-77c5-4dd5-bae8-89aafbd89b99",
   "metadata": {},
   "source": [
    "This notebook is for doing Keyword in Context analyses for multiple files in a directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315be910-bff6-4d24-99c5-91d518b88423",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Using spaCy to pull out sentences that contain a given keyword\n",
    "import spacy\n",
    "import re\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "from pathlib import Path  \n",
    "import glob\n",
    "\n",
    "\n",
    "#Download the language model you're interested in\n",
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9632f49e-17af-4582-a78d-766220fea786",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Load language model\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "#Open your texts and create spaCy document\n",
    "filepath = 'kafka-corpus/'\n",
    "text_files = glob.glob(f'{filepath}/*.txt')\n",
    "\n",
    "\n",
    "def find_sentences_with_keyword(keyword, document):\n",
    "        for sentence in document.sents:\n",
    "            sentence = sentence.text\n",
    "            if keyword.lower() in sentence.lower():\n",
    "                #Use the regex library to replace linebreaks and to make the keyword bolded, ignoring capitalization\n",
    "                sentence = re.sub('\\n', ' ', sentence)\n",
    "                sentence = re.sub(f\"{keyword}\", f\"**{keyword}**\", sentence, flags=re.IGNORECASE)\n",
    "                display(Markdown(sentence))\n",
    "\n",
    "#Loop through the files and open as spacy document\n",
    "#Then print for each document the sentences containing a given keyword\n",
    "#Change keyword ('france') to keyword you are looking for\n",
    "for file in text_files:\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "        print(file)\n",
    "        document = nlp(text)\n",
    "        kwic = find_sentences_with_keyword(keyword='legs', document=document)\n",
    "        print(kwic)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e240a0-2dbb-4707-871d-2443d84506e1",
   "metadata": {},
   "source": [
    "# Keywords in Context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363f87fb-5058-4389-96f5-9bbd0e5f7067",
   "metadata": {},
   "source": [
    "n-grams: any sequence of n tokens, n number of words  \n",
    "N-grams are a way of representing groups of words as a single token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa999d7f-9ea5-4053-9637-0129620ba48d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Loop through files and tokenize\n",
    "directory_path = 'kafka-corpus/'\n",
    "all_docs = []\n",
    "\n",
    "def tokenize(text):\n",
    "    lowercase_text = text.lower()\n",
    "    split_words = re.split(r'\\W+', lowercase_text)\n",
    "    tokenized = [word for word in split_words if word.isalpha()]\n",
    "    return tokenized\n",
    "\n",
    "for filepath in Path(directory_path).glob(\"*.txt\"):\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "        tokenized_text = tokenize(text)\n",
    "        all_docs.append(tokenized_text)\n",
    "\n",
    "all_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297fcbc3-7845-48ac-9e44-54f859a2dfc4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Remove stopwrds\n",
    "#Stopwords: refer to \"Preprocessing\" notebook for more details on stopwords\n",
    "\n",
    "#Load custom stopwords list (this is the default spacy list)\n",
    "#open your txt file and convert to a Python list\n",
    "with open(\"custom-stopwords.txt\", \"r\") as file_object:\n",
    "    custom_stopwords = [s.rstrip('\\n') for s in file_object.readlines()] \n",
    "\n",
    "custom_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca9259f-0dab-4e5f-ba27-8a56c66d6e75",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Define a function to remove stopwords from tokens\n",
    "def remove_stopwords(list_of_tokens, stopwords):\n",
    "    return [token for token in list_of_tokens if token not in stopwords]\n",
    "\n",
    "all_docs_no_stop = []\n",
    "\n",
    "for file in all_docs: \n",
    "    nostop = remove_stopwords(file, custom_stopwords)\n",
    "    all_docs_no_stop.append(nostop)\n",
    "    \n",
    "all_docs_no_stop[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3367bdcd-9e45-409f-bac5-4eddcf3ea1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a function to return list of ngrams\n",
    "def make_ngrams(tokens, n):\n",
    "    ngrams = []\n",
    "    for i in range(len(tokens)-(n-1)):\n",
    "        ngrams.append(tokens[i:i+n])\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bec3c8-d126-4898-b111-e1c28de42a8c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Displays words that appear around a particular word\n",
    "\n",
    "#Define a function to create a dictionary from n-grams, using middle word as the key.\n",
    "#To figure out the keyword for each n-gram we can use the index positions of the list.\n",
    "def ngrams_to_dictionary(ngrams):\n",
    "    keyindex = len(ngrams[0]) // 2\n",
    "\n",
    "    ngram_dictionary = {}\n",
    "\n",
    "    for ngram in ngrams:\n",
    "        if ngram[keyindex] not in ngram_dictionary:\n",
    "            ngram_dictionary[ngram[keyindex]] = [ngram]\n",
    "        else:\n",
    "            ngram_dictionary[ngram[keyindex]].append(ngram)\n",
    "    return ngram_dictionary\n",
    "\n",
    "keywords = []\n",
    "\n",
    "#Loop through the files and append the dictionaries for each file\n",
    "#to a list called keywords\n",
    "#Change the number (6) to change the size of the window of words around the keyword\n",
    "for file in all_docs_no_stop:\n",
    "        ngrams = make_ngrams(file, 6)\n",
    "        keywords_in_context = ngrams_to_dictionary(ngrams)\n",
    "        keywords.append(keywords_in_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd3fe9f-de0b-4c26-ad7c-45b952149d17",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Define a function that will loop through the list of ngram dictionaries\n",
    "#and print out a given keyword with its ngrams\n",
    "#or print a line that the keyword is not in the dictionary\n",
    "def lookup_keyword(kw, dictionaries):\n",
    "    for i in range(len(dictionaries)):\n",
    "        text_name = text_files[i]\n",
    "        dictionary = dictionaries[i]\n",
    "        if kw in dictionary:\n",
    "            print(text_name, dictionary[kw], \"\\n\")\n",
    "        else:\n",
    "            print(f\"{kw} not in file: {text_name}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc28754f-f9c6-4d5b-ac72-c11dbc9aec9c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Look up the words that appear next to a given keyword\n",
    "lookup_keyword('people', keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873169ec-a111-4ad0-b19b-c255fded5e4a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Look up most frequent words that appear next to a given keyword\n",
    "\n",
    "#Make a function to return most frequent words that appear next to a particular keyword\n",
    "from collections import Counter\n",
    "def get_neighbor_words(keyword, ngrams):\n",
    "    \n",
    "    neighbor_words = []\n",
    "    keyword = keyword.lower()\n",
    "    \n",
    "    for ngram in ngrams:\n",
    "        if keyword in ngram:\n",
    "            for word in ngram:\n",
    "                if word != keyword:\n",
    "                        neighbor_words.append(word)\n",
    "    return Counter(neighbor_words).most_common()\n",
    "\n",
    "all_ngrams = []\n",
    "\n",
    "#Loop through the files and append the ngrams for each file\n",
    "#to a list called all_ngrams\n",
    "#If you want more nearest neighbours increase the number '2'\n",
    "#which increases the window of ngrams\n",
    "for file in all_docs_no_stop:\n",
    "    ngrams = make_ngrams(file, 2)\n",
    "    all_ngrams.append(ngrams)\n",
    "\n",
    "#Define a function to loop through ngrams above\n",
    "# and look up most common neighbor words for a given keyword\n",
    "def lookup_neighbor_words(keyword, ngram_list):\n",
    "    for i in range(len(ngram_list)):\n",
    "        text_name = text_files[i]\n",
    "        text_ngrams = ngram_list[i]\n",
    "        print(text_name, get_neighbor_words(keyword, text_ngrams), '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b14460-da69-44bb-8eee-82777685fc0c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Look up the most common neighbor words for a given keyword\n",
    "lookup_neighbor_words('people', all_ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7467637d-d32c-4f81-95b0-5a5a73edb023",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
