{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13e5f108-23e1-476f-8a54-8890704326e2",
   "metadata": {},
   "source": [
    "# Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3853aa1-98e0-4a24-86b0-f6662a1d5105",
   "metadata": {},
   "source": [
    "**Contents**\n",
    "\n",
    "[What is preprocessing and why is it important?](#section-1)\n",
    "\n",
    "[Tokenizing](#section-2)\n",
    "\n",
    "[Lowercasing and Punctuation](#section-3)\n",
    "\n",
    "[Normalization](#section-4)\n",
    "\n",
    "[Lemmatization and Stemming](#section-5)\n",
    "\n",
    "[Textual units of analysis: chunking](#section-6)\n",
    "\n",
    "[Stopwords](#section-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e13f6de-7f58-4a5a-8979-adcb5176fda9",
   "metadata": {},
   "source": [
    "<a id='section-1'></a>\n",
    "## What is preprocessing and why is it important?  \n",
    "\n",
    "Preprocessing are steps and procedures to take before using analyzing the texts in order to prepare your text data for analysis and make the results more meaningful. Text analysis methods require data to be structured in certain ways and will work better if the text data are prepared in certain ways. For example, most text analysis methods rely on matching the same sequences of characters in order to count them together, the more consistant the data is - the more the things we want counted together share the same sets of characters, the same spelling, etc - the better the outputs will be.  \n",
    "\n",
    "In sum, preprocessing is about modifying and preparing the text data so that they work well with the methods you intend to use. Preprocessing steps are especially important when working with languages that are not Engish or that have features very different from English since there might be particular considerations and steps to take so that the text data work well with widely used text analysis methods. \n",
    "\n",
    "There is no one-size-fits-all for preprocessing. Different methods, text data, questions and stages of anlaysis might require different kinds of preprocessing choices. \n",
    "\n",
    "**Why is preprocessing important?**\n",
    "\n",
    "The decisions made and the steps taken during preprocessing change influences the analysis and the outputs generated. And yet, as Nguyen et al. point out, preprocessing steps are often underreported and overlooked.\n",
    "\n",
    ">“The pre-processing steps have a big impact on the operationalizations, subsequent analyses and reproducibility efforts (Fokkens et al., 2013), and they are usually tightly linked to what we intend to measure. Unfortunately, these steps tend to be underreported, but documenting the pre-processing choices made is essential” (Nguyen et al. “How We Do Things With Words”, p. 7)\n",
    "\n",
    "\n",
    "It is also not always evident what the consequences our preprocessing chocies might be, which makes it even more crucial to be explicit about preprocessing procedures and document the steps taken. \n",
    "\n",
    "> “In unsupervised settings, it is more challenging to understand the effects of different steps. Inferences drawn from unsupervised settings can be sensitive to pre-processing choices (Denny and Spirling, 2018). (...) All in all, this again highlights the need to document these steps.” (Nguyen et al. “How We Do Things With Words”, p. 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8cabb1-4fd0-45db-93a4-7ae45098c85a",
   "metadata": {},
   "source": [
    "<a id='section-2'></a>\n",
    "## Tokenizing\n",
    "\n",
    "Tokenizing involves splitting the text into units of analysis you're interested in analyzing - most often this is assumed to be \"words\".\n",
    "\n",
    "Why do we need to do this? Look what happens when we pass \"raw text\", an untokenized text, to Counter (in order to count occurrences in our text):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb686c2-4618-4eec-93d2-c98c64a14ef3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "f = open('kafka_dv.txt', 'r')\n",
    "test_text = f.read()\n",
    "Counter(test_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2106ffae-1267-4b80-a9b9-8ad9f6f0ad87",
   "metadata": {},
   "source": [
    "It counts every single character. Strings/text are sequences of character encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdd3c62-cbf3-4027-93e8-5bbffd0c9e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ord() returns the unicode code-point value in Python of a character\n",
    "ord('å')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f399bb-5048-4a2a-9a4e-2f483e100f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#chr() returns the character associated with the code-point\n",
    "chr(229)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3868e7-a1d6-47f3-8df0-8466aa31eccd",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#This is how computers read and store information\n",
    "#prints the sequences of character code-points that are encoded/deconded into bits\n",
    "text = 'It was the best of times, it was the worst of times.'\n",
    "\n",
    "for char in range(len(text)):\n",
    "    print(ord(text[char]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997b9aeb-1913-4c7a-a7ca-b2b23b0a1b2f",
   "metadata": {},
   "source": [
    "We need to split out the sequences of characters into units we want to analyze. This process is called tokenizing: restructuring the text data into units we want to analyze."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11617f10-c8a7-4437-a70a-8c67fe25a2db",
   "metadata": {},
   "source": [
    "Tokenization works by defining markers at which you split the string. Different tokenizing procedures might use different markers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed0d531-e828-4d46-9c8d-e24ac591571b",
   "metadata": {},
   "source": [
    "Here are different ways of tokenizing text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb605254-f193-445c-af44-7cedb3025564",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the .split() method in Python uses whitespace as default\n",
    "text = \"I'd say, they're happy it's mother's day.\"\n",
    "text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0d8b94-f80a-4118-a9be-52d27854fe24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#you can also pass different markers to .split() to define where you want to split your text\n",
    "#this uses regular expression to split at any one character or more that is NOT a word\n",
    "import re\n",
    "text = \"I'd say, they're happy it's mother's day.\"\n",
    "tokens = re.split('\\W+', text)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b3e85e-0137-4791-a785-d0c53f15abd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Built-in tokenizing procedure in NLTK\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "text = \"I'd say, they're happy it's mother's day.\"\n",
    "tokens = nltk.word_tokenize(text)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1aff5d-6afb-4c6a-8da8-bec47053c743",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Built-in tokenizing procedure in spaCy in Spanish\n",
    "#Download model\n",
    "import spacy\n",
    "!python -m spacy download es_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be74d1c-6c9d-47c5-9b6e-208353da7a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load language model\n",
    "nlp = spacy.load('es_core_news_sm')\n",
    "#Create spaCy process document\n",
    "text = 'Yo diría, que están felices de que sea el día de la madre.'\n",
    "document = nlp(text)\n",
    "\n",
    "tokens = [token.text for token in document]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3890cdb1-ba40-4660-8884-709ebf02e024",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Built-in tokenizing procedure in spaCy in French\n",
    "#Download model\n",
    "import spacy\n",
    "!python -m spacy download fr_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f13fd23-1fa6-4e75-ad59-4ebc11251d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load language model\n",
    "nlp = spacy.load('fr_core_news_sm')\n",
    "#Create spaCy process document\n",
    "text = 'On dirait qu\\'ils sont heureux que ce soit la fête des mères.'\n",
    "document = nlp(text)\n",
    "\n",
    "tokens = [token.text for token in document]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9116cf-b7da-46a3-8a9f-c941336992d5",
   "metadata": {},
   "source": [
    "**Define your own tokenizing function**\n",
    "\n",
    "You could define your own tokenizing function to tokenize your text the way you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d9f85a-edd6-4907-9201-008d07b0be76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only words, no numbers\n",
    "#Define a function to lowcase, split at and remove anything not a \"word\" character\n",
    "#(i.e. a letter or digit or underbar)\n",
    "#So it will split at and remove whitspace and punctuation\n",
    "#Then keep only alphabetic characters (i.e. remove numbers) with .isalpha()\n",
    "\n",
    "def tokenize(text):\n",
    "    lowercase_text = text.lower()\n",
    "    split_words = re.split('\\W+', lowercase_text)\n",
    "    no_numbers = [word for word in split_words if word.isalpha()]\n",
    "    return no_numbers\n",
    "\n",
    "text_example = \"I'd say, they're happy it's mother's day. 1988!\"\n",
    "tokenized_text_example = tokenize(text_example)\n",
    "tokenized_text_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca64721-c40f-4c8a-8ea9-88f9d2ab78a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Words and numbers\n",
    "#Define a function to lowcase, split at and remove anything not a \"word\" character\n",
    "#(i.e. a letter or digit or underbar)\n",
    "#So it will split at and remove whitspace and punctuation\n",
    "\n",
    "def tokenize(text):\n",
    "    lowercase_text = text.lower()\n",
    "    split_words = re.split('\\W+', lowercase_text)\n",
    "    return split_words\n",
    "\n",
    "text_example = \"I'd say, they're happy it's mother's day. 1988!\"\n",
    "tokenized_text_example = tokenize(text_example)\n",
    "tokenized_text_example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4fef46-8dd8-4801-bcdd-ebb841cf7e66",
   "metadata": {},
   "source": [
    "Tokenizing procedures are therefore premised on the assumption that meaningful semantic units are separable by repeated markers such as whitespace or punctuation.\n",
    "\n",
    "Different tokenizing procedures operationalize different assumptions about what the markers that delimit semantic units should be (for example, predominently relying on the assumption that meaningful semantic units are words separated by whitespace). \n",
    "\n",
    "Some critics resist the assumption that the primary meaningful units of analysis should be words. And if words really are the most meaningful units of analysis, then it might not always be clear what a word is. \n",
    "\n",
    "For example, Ramsay argues that \n",
    "> “Tokenization forces us to confront the fact that the notion of a word is neither unambiguous nor satisfactorily definable for all circumstances.” (Ramsay, _Reading Machines_, p. 34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c178e4a-95a0-482a-a87a-1570bca7be52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#White space does not always signal the demarcation between two different semanic units\n",
    "#For example, in Vietnamese, the word \"thời gian\" is read as a single semantic unit\n",
    "#but it would be split according to most default tokenizing procedures\n",
    "text = \"thời gian\"\n",
    "text.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d509942-1b3f-4068-a402-7bd207991b12",
   "metadata": {},
   "source": [
    "Nguyen et al. also raise the issue of multi-words: \n",
    "\n",
    ">“Multi-word terms are also challenging. Treating them as a single unit can dramatically alter the patterns in text. Many words that are individually ambiguous have clear, unmistakable meanings as terms, like “black hole” or “European Union.”” (Nguyen et al. “How We Do Things With Words”, p. 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6dcc157-2c17-4094-881e-9d7364362f80",
   "metadata": {},
   "source": [
    "Tokenization is also problematic for agglutinative languages, languages that generate words and sentences by combining units together. \n",
    "\n",
    "For example, in German, words are often combined together to create new concepts or simply to express ideas in more compact form. \n",
    "\n",
    "For example: \n",
    "\n",
    "Kummerspeck = Kummer (grief, sorrow) + Speck (bacon) = the weight you put on from emotional overeating\n",
    "\n",
    "Dekadenzkonzept = Dekadenz (decadence) + Konzept (concept) = the concept of decadence\n",
    "\n",
    "This last word would be tokenized into separate entities in English, but there is no easy way to do that in German, and it's not clear if we should do that in the first place - is this combined word a distinct concept or should it be decomposed into its constitutent forms? Is it simply the addition of its constitutent units?\n",
    "\n",
    "Tokenizing involves decisions and choices that may impact the analysis. \n",
    "\n",
    "> “Such choices may appear simple, but they may have a strong influence on the final text representation, and, subsequently, on the analysis based on this representation.” (Karsdorp, et al. _Humanities Data Analysis_, p. 82)\n",
    "\n",
    "\n",
    ">“One step that almost everyone takes is to tokenize the original character sequence into the words and word-like units. Tokenization is a more subtle and more powerful process than people expect.” (Nguyen et al. “How We Do Things With Words”, p. 8)\n",
    "\n",
    "\n",
    "> “Unfortunately, it is difficult to provide a recommendation here apart from advising that tokenization procedures be carefully documented.” (Karsdorp, et al. _Humanities Data Analysis_, p. 82)\n",
    "\n",
    "\n",
    "> “tokenizers may come with a certain set of assumptions, which should be made explicit through, for instance, properly referring to the exact tokenizer applied in the analysis.” (Karsdorp, et al. _Humanities Data Analysis_, p. 83)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3ef554-cd51-4855-b15d-4504c6ae4c69",
   "metadata": {},
   "source": [
    "**Segmentation**  \n",
    "\n",
    "Some languages do not separate words with spaces. One way to tokenize for these language is to artificially insert spaces in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e01d688-7f0a-40e0-9879-688d782bc628",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Segmentation for Chinese in spaCy\n",
    "import spacy\n",
    "#!python -m spacy download zh_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a82e47-5def-4622-8ca7-de0675c2f3c8",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Load language model\n",
    "nlp = spacy.load('zh_core_web_sm')\n",
    "#Create spaCy processed document\n",
    "filepath = 'segmentation_text_sample.txt'\n",
    "text = open(filepath, encoding='utf-8').read()\n",
    "document = nlp(text)\n",
    "\n",
    "# Create a segmented version of the original text file\n",
    "#Loop through each token in the original text, lemmatize and lowercase each token, \n",
    "#and insert a space between the tokens. Then write them out to new file\n",
    "\n",
    "outname = filepath.replace('.txt', '-segmented.txt')\n",
    "with open(outname, 'w', encoding='utf8') as out:\n",
    "    for token in document:\n",
    "        print(token)\n",
    "        out.write(repr(token))\n",
    "        out.write(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5097a7c9-4ab7-4ddb-a466-35edeb2a0830",
   "metadata": {},
   "source": [
    "<a id='section-3'></a>\n",
    "## Lowercasing and Punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a980694-a9e8-4b35-9a7f-563e321e0b52",
   "metadata": {},
   "source": [
    "Uppercases and lowercases have distinct encodings and will be counted as distinct characters. Similarly, punctuation is an encoded character - words with trailing punctuation will be counted as distinct characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1dea58-eda5-4eec-b456-a43937fc70f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "example_text = 'And then, there were none. None and then there were.'\n",
    "tokenized_example_text = example_text.split()\n",
    "Counter(tokenized_example_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55e5fa2-224a-4dfa-bc9b-03dd3a11ba7b",
   "metadata": {},
   "source": [
    "It is common practice to lowercase the text and remove punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e80652e-dc3a-4955-bd0b-3952bbdc9c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lowercase using .lower() string method\n",
    "lower_example_text = example_text.lower()\n",
    "lower_example_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a1e110-9bbe-4a75-aeef-c69dc63a3635",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use isaplha() to assess if each word is an alphabetic character\n",
    "#will remove punctuation and numbers\n",
    "nopunct_text = [word for word in tokenized_example_text if word.isalpha()]\n",
    "nopunct_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784213a0-477c-49b8-ad8a-6b4083b9460c",
   "metadata": {},
   "source": [
    "Even these routine practices could have consequences for analysis. Upper cases and lower cases carry semantic meaning. For example, in German nouns are capitalized so \"Essen\" means \"food\" and \"essen\" means \"to eat\". We lose that distinction by lowercasing our text. \n",
    "\n",
    "> “We already spoke about lowercasing texts, which is another common preprocessing step. Here as well, we should be aware that it has certain consequences for the final text representation. For instance, it complicates identifying proper nouns or the beginnings of sentences at a later stage in an analysis.” (Karsdorp, et al. _Humanities Data Analysis_, p. 83)\n",
    "\n",
    "Similarly, punctuation marks can also be carriers of semantic meaning (as analyzed by Piper in Andrew Piper, \"Punctuation (Opposition)\" in *Enumerations*, The University of Chicago Press, 2018). Depending on our research goals, it may be necessary to consider the semantic function played by punctuation.\n",
    "\n",
    "> “To illustrate the complexity, consider the problem of modeling thematic differences between texts. For this problem, certain linguistic markers such as punctuation might not be relevant. However, the same linguistic markers might be of crucial importance to another problem. In authorship attribution, for example, it has been demonstrated that punctuation is one of the strongest predictors of authorial identity (Grieve 2007).” (Karsdorp, et al. _Humanities Data Analysis_, p. 83)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ef5bf3-cb4c-47df-a4ed-74c987799d4c",
   "metadata": {},
   "source": [
    "<a id='section-4'></a>\n",
    "# Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d38b31c-77eb-4c21-923a-7b886f65715f",
   "metadata": {},
   "source": [
    "Normalization is a catch-all term for a number of different procedures that mainly have to do with the idea of reducing inconsistencies and variations in the text data. \n",
    "\n",
    "Text analysis methods work better the more consistent the text data are: because methods rely on matching sequences of characters, the more consistent the sequences of characters you want matched, the better the results. \n",
    "\n",
    "Normalization could involve smoothing over inconsistencies in spelling - either because different forms of spelling exist, or because there are spelling errors introduced by OCR. \n",
    "\n",
    "> “Data may also vary enormously in quality, depending on how it has been generated. Many historians, for example, work with text produced from an analog original using Optical Character Recognition (OCR). Often, there will be limited information available regarding the accuracy of the OCR, and the degree of accuracy may even vary within a single corpus (e.g., where digitized text has been produced over a period of years, and the software has gradually improved). The first step, then, is to try to correct for common OCR errors. These will vary depending on the type of text, the date at which the “original” was produced, and the nature of the font and typesetting.” (Nguyen et al. “How We Do Things With Words”, p. 7)\n",
    "\n",
    "Normalization, like other pre-processing procedures, therefore also involves making decisions about what  is considered a meaningful variation and what is not. \n",
    "\n",
    "> “Each step requires making additional assumptions about which distinctions are relevant: is “apple” different from “Apple”? Is “burnt” different from “burned”? Is “cool” different from “coooool”? Sometimes these steps can actively hide useful patterns, like social meaning (Eisenstein, 2013). Some of us therefore try do as little modification as possible.” (Nguyen et al. “How We Do Things With Words”, p. 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b263e833-2c03-4ced-a512-58fb72b074ed",
   "metadata": {},
   "source": [
    "Indentifying characters to normalize using characters counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404e2d27-440c-4f21-9683-82573ddfa8ff",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Counting characters in a text\n",
    "from collections import Counter\n",
    "\n",
    "f = open('kafka_dv.txt', 'r')\n",
    "test_text = f.read()\n",
    "Counter(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1806d1ce-3399-46f0-83a3-429ef7f3b3b4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Replacing problematic characters\n",
    "test_text.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f337cf80-0aeb-494b-830a-18a939e29bb4",
   "metadata": {},
   "source": [
    "**Normalizing encodings**\n",
    "\n",
    "\n",
    "Character counts can also flag up any encoding issues. Especially when working with language that have accented characters, we can see here if there are any problems with those accented characters (for example, if the \"same\" accented characters don't in fact share the same encoding). Then we can normalize the data to ensure all characters are encoded properly so that it doesn't mess up our counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc74504-bf07-45f4-b44e-88fc5e43eafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For example, here we have the letter \"a\" with acute accent\n",
    "# Its unicode value in python is 225\n",
    "#and it has a length of 1: it is one accented character\n",
    "import unicodedata\n",
    "char = \"á\"\n",
    "print(ord(char))\n",
    "len(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72197184-9aea-4b0d-803b-d96a3212a8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The unicode name for this character\n",
    "[ unicodedata.name(c) for c in char ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1912c7-4dcd-42b1-adce-b04e5b552a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#But a character that looks the same could be encoded differently\n",
    "#Here we have the letter a with acute accent again\n",
    "#but it is in fact the combination of two unicode code-points (97 + 769) \n",
    "#so it has a length of 2\n",
    "char2 = \"á\"\n",
    "print([ord(c) for c in char2])\n",
    "len(char2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86dbe26-eeae-4cc1-9dbf-3c0182da4931",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The unicode name for this character\n",
    "[ unicodedata.name(c) for c in char2 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac14d4ca-ace6-4cb1-a8aa-aa1c973a12ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#These characters look the same but are not infact the same, and have different lengths\n",
    "len(char) == len(char2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552b378a-7e6b-4e91-8c37-86e3593088e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Different character encodings will be counted as different characters\n",
    "'hálo friend' == 'hálo friend'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51478be9-678c-4c19-8b41-ef30cd94e113",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can use the .normalize() method to normalize the character encodings\n",
    "test = char + char2\n",
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e33bd12-692c-4ed1-b596-6942f97572ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize all character encodings in your text\n",
    "new_test = unicodedata.normalize('NFC', 'test')\n",
    "len(new_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4811f3b-daf1-451c-a31c-a88c21153461",
   "metadata": {},
   "source": [
    "Identifying words to normalize using word counts and `.replace()` as above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c39a1f-555c-40cd-9602-a7b4be42ee74",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Counting all the words in a text\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "#defining a tokenizing function which will split at and remove whitespace and punctuation\n",
    "#return words and numbers\n",
    "def tokenize(text):\n",
    "    lowercase_text = text.lower()\n",
    "    split_words = re.split('\\W+', lowercase_text)\n",
    "    return split_words\n",
    "\n",
    "#Read in text\n",
    "text = open('kafka_metamorphosis.txt', encoding=\"utf-8\").read()\n",
    "\n",
    "#Use our tokenizing function to tokenize the text\n",
    "all_the_words = tokenize(text)\n",
    "\n",
    "##Count frequencies of all the words\n",
    "all_the_words_count = Counter(all_the_words)\n",
    "all_the_words_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e9a8a5-58bd-4cd1-bdd9-e3267f5c78cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1e83b5e5-dc60-4a2f-8f07-79a056a302c4",
   "metadata": {},
   "source": [
    "<a id='section-5'></a>\n",
    "# Lemmatization and Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771a1648-351b-45ae-865a-84bb6f2021e8",
   "metadata": {},
   "source": [
    "Both stemming and lemmatization are procedures that reduce the inflectional forms of words to a common base or root. \n",
    "\n",
    "English has minimal inflection (e.g. words can be inflected by number: \"cat\" becomes \"cats\" in the plural). Other languages, however, have much more inflection. Words can vary, for example, according to whether the word is definite or indefinite, and also according to number and gender. For example in Swedish, not only could there be three different forms for \"little\" - \"litet\", \"liten\", \"lilla\", but there are also four different forms for \"coat\" (jacka, jackan, jackor, jackorna) and \"apple\" (äpple, äpplet, äpplen, äpplena), depending on gender and whether they’re plural or singular and definite or indefinite (whereas in English there would only be two different forms: plural and singular). When working with methods that rely on word counts, and on grouping words we consider the \"same\" together, these inflectional forms need to be grouped together. Stemming and lemmatization are two processes to reduce inflectional forms to a base form, but they do it in slightly different ways."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2458c70-0249-4588-9ed3-6728d3b512f4",
   "metadata": {},
   "source": [
    "### Stemming  \n",
    "\n",
    "Stemming converts a word into a stem: it works with a list of common inflectional prefixes and suffixes specific to a language, and it cuts off the beginnings and endings of words that have those forms. \n",
    "\n",
    "> sing > sing  \n",
    "\n",
    ">singing > sing  \n",
    "\n",
    ">sung > sung  \n",
    "\n",
    ">sang > sang\n",
    "\n",
    "\n",
    "\n",
    ">niñas > niñ   \n",
    "\n",
    ">niñez > niñ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ceeda6-a86a-4413-9dc2-310f8b23ebc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming with NLTK\n",
    "#Imports\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "text = 'To sing a happy song, while dancing a happy dance, is to experience happiness, one of the best experiences.'\n",
    "\n",
    "#tokenize\n",
    "tokens = nltk.word_tokenize(text)\n",
    "\n",
    "#initialize stemmer\n",
    "porter = PorterStemmer()\n",
    "\n",
    "#stem tokens\n",
    "stemmed_tokens = [porter.stem(token) for token in tokens]\n",
    "\n",
    "#count what words counted together\n",
    "counts = Counter(stemmed_tokens)\n",
    "counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e5218c-8c58-4dbd-83d9-a4920c4e8445",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "\n",
    "Lemmatization converts inflections to a base lemma. Lemmatization takes into account morhpology (how words change when inflected) and rely on dictionaries to identify the base lemma of different forms. \n",
    "\n",
    "> sing > sing \n",
    "\n",
    "> singing > sing \n",
    "\n",
    "> sung > sing \n",
    "\n",
    "> sang > sing  \n",
    "\n",
    "> niñas > niño  \n",
    "\n",
    "> niñez > niñez\n",
    "\n",
    "Lemmatization might be more precise at grouping together words we think belong together whilst stemming might be more imprecise - it might not take into account all the different inflections of a word, and it might group together words that might not necessarily belong together. Yet stemming can still be an efficient procedure for low inflection languages. Furthermore, as Nguyen et al. point out, not all languages have the same kinds of resources available, and there might not be good quality lemmatization packages available.\n",
    "\n",
    "\n",
    "> “From a multilingual perspective, English and Chinese have unusually simple inflectional systems, and so it is statistically reasonable to treat each inflection as a unique word type. Romance languages have considerably more inflections than English; many indigenous North American languages have still more. For these languages, unseen data is far more likely to include previously-unseen inflections, and therefore, dealing with inflections is more important. On the other hand, the resources for handling inflections vary greatly by language, with European languages dominating the attention of the computational linguistics community thus far.” (Nguyen et al. “How We Do Things With Words”, p. 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4266b417-30e9-4023-b518-02a131897e88",
   "metadata": {},
   "source": [
    "**Lemmatizing mutiple files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba56cf6-b990-4e11-bec9-0dcb1bee1bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This loops over multiple files in a directory\n",
    "#but it might make the kernel crash if it runs out memory\n",
    "#If the kernel crash you might have to lemmatize single files at a time (cf. below)\n",
    "\n",
    "#Lemmatizing using spaCy for English\n",
    "import spacy\n",
    "import glob\n",
    "\n",
    "#Download the language model you're interested in (this is the English pipeline)\n",
    "#For french: fr_core_news_sm\n",
    "#For spanish: es_core_news_sm\n",
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5dee31-b5df-40f0-a3ea-29e479da9330",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load language model (it needs to match the name above)\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556d902f-41d2-4963-a701-6abe6825288b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Open your texts and create spaCy document\n",
    "filepath = 'kafka-corpus/'\n",
    "text_files = glob.glob(f'{filepath}/*.txt')\n",
    "\n",
    "#Loop through the files and open as spacy document\n",
    "for file in text_files:\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "        print(file)\n",
    "        document = nlp(text)\n",
    "        \n",
    "    #Lemmatize\n",
    "    outname = file.replace('.txt', '-lemmatized.txt')\n",
    "    with open(outname, 'w', encoding='utf8') as out:   \n",
    "        for token in document:\n",
    "            # Get the lemma for each token\n",
    "            out.write(token.lemma_.lower())\n",
    "            # Insert white space between each token\n",
    "            out.write(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d620a6-c6ce-4eda-9315-cb5cf11022d9",
   "metadata": {},
   "source": [
    "**Lemmatizing single files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18037dac-afc0-481a-b442-6ff6215d24e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatizing single files\n",
    "\n",
    "#Lemmatizing using spaCy for English\n",
    "import spacy\n",
    "#!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12aef9fe-a1a2-4dc2-9fc3-b676f3adc2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load language model (it needs to match the name above)\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "#Open your text and create spaCy document\n",
    "filepath = 'kafka_metamorphosis.txt'\n",
    "text = open(filepath, encoding='utf-8').read()\n",
    "document = nlp(text)\n",
    "\n",
    "outname = filepath.replace('.txt', '-lemmatized.txt')\n",
    "with open(outname, 'w', encoding='utf8') as out:   \n",
    "    for token in document:\n",
    "        # Get the lemma for each token\n",
    "        out.write(token.lemma_.lower())\n",
    "        # Insert white space between each token\n",
    "        out.write(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a44427-0a12-4180-823f-6f8a5c57b48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prints the original word in the text, \n",
    "#a dash, then the lemmatized form that was written to the derivative text document\n",
    "#check if there are places where the model consistently makes mistakes\n",
    "#this prints the first 50 tokens - modify the slice next to document for more\n",
    "for token in document[:50]:\n",
    "    print(token.text + ' - ' + token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3e1ca6-b3ba-487a-97a5-8d02a0e5ff54",
   "metadata": {},
   "source": [
    "<a id='section-6'></a>\n",
    "# Textual units of analysis: chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4d4da5-7513-4b81-868f-09b7ab54e3fc",
   "metadata": {},
   "source": [
    "Having texts of widely different lengths might skew the analyses. It's good practice to ensure that the texts are of roughly similar lengths. This might mean joining texts together into larger text blocks if they are very short (e.g. tweets), or splitting longer texts into shorter units.\n",
    "\n",
    "This process can be called chunking. It is also sometimes referred to as segmentation.\n",
    "\n",
    "> From a computational perspective, the unit of text can also make a huge difference, especially when we are using bag-of-words models, where word order within a unit does not matter (Boyd-Graber et al., 2017). Finding a good segmentation sometimes means combining short documents and subdividing long documents.” (Nguyen et al. “How We Do Things With Words”, p. 6)\n",
    "\n",
    "\n",
    "> “Small segments, like tweets, sometimes do not have enough information to make their semantic context clear (Mehrotra et al., 2013). In contrast, larger segments, like novels, have too much variation, making it difficult to train focused models (Jockers, 2013). The word “document” can therefore be misleading. But it is so ingrained in the common NLP lexicon that we use it anyway in this article.” (Nguyen et al. “How We Do Things With Words”, p. 6-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa87e043-732b-4d2b-a568-08f2ee823af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loop through and check how long the texts are\n",
    "import glob\n",
    "\n",
    "#Open your texts\n",
    "filepath = 'kafka-corpus/'\n",
    "text_files = glob.glob(f'{filepath}/*.txt')\n",
    "\n",
    "#Loop through the files and print text file name with number of words\n",
    "for file in text_files:\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "        print(file, len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33848a89-4acb-4c68-80fa-59cf692a362c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split long texts into shorter units\n",
    "#Split into a collection of documents of 3000 words\n",
    "\n",
    "#Loop through the files and print text file name with number of words\n",
    "for file in text_files:\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "        print(file, len(text))\n",
    "        \n",
    "        segment_length = 3000\n",
    "        \n",
    "        nseg = round(len(text) / segment_length)\n",
    "        for i in range(nseg):\n",
    "            segment = text[segment_length*i:segment_length*(i+1)]\n",
    "            outname = file.replace('kafka-corpus/', 'kafka-corpus/kafka-segmented/').replace('.txt', f'-{i}.txt')\n",
    "            with open(outname, 'w', encoding='utf8') as out:\n",
    "                text_chunk = ''.join(segment)\n",
    "                out.write(text_chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2eb839-314d-4019-8f29-1c4040501d3e",
   "metadata": {},
   "source": [
    "Note that this is a coarse kind of splitting - it can split in the middle of words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a834b41-803c-4152-8917-5fce003609e6",
   "metadata": {},
   "source": [
    "<a id='section-7'></a>\n",
    "# Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc14e693-bcff-4e2d-9040-51b8c4be32b3",
   "metadata": {},
   "source": [
    "Stopwords are lists of words that we want to filter out from our analyses because they are considered irrelevant or not meaningful to our analyses. Stopwords are important because any word that is on the stopword list will be removed from the analyses. \n",
    "\n",
    "There is no real agreement of what should or should not be included on a stopwords list, and this varies widely depending on the research aims and questions. \n",
    "\n",
    "Very often, stopwords list include function words. Function words are used to express grammatical relations and hold sentences together. They include pronouns (e.g. I, she, he, me, you, they, their, him, her), articles (e.g. the, a), conjunctions (e.g. and, before, but, because, for, whether, that), and prepositions (e.g. below, before, in, during). Function words are grammatically useful, but are considered not to carry much semantic content. In contrast, content words, such as verbs (e.g. run, shout, eat), nouns (e.g. apple, sister, depth), and adjectives (e.g. yellow, quiet, condusing) are considered as carrier of meaning. Yet function words are effective indicators of style - function words are therefore central to analyses of style and authorship. The choices made about what words to include on a stopwords list will therefore vary according to the specificities of the research questions and the text data.\n",
    "\n",
    ">“We sometimes also remove words that are not relevant to our goals, for example by calculating vocabulary frequencies. We construct a “stoplist” of words that we are not interested in. If we are looking for semantic themes we might remove function words like determiners and prepositions. If we are looking for author-specific styles, we might remove all words except function words. Some words are generally meaningful but too frequent to be useful within a specific collection. The word “prisoner” would be very interesting in most contexts, but in London court records that consist entirely of decisions about prisoners, it adds nothing. We sometimes also remove very infrequent words. Their occurrences are too low for robust patterns and removing them helps reducing the vocabulary size.”\n",
    "(Nguyen et al. “How We Do Things With Words”, p. 8)\n",
    "\n",
    "\n",
    ">“What one researcher considers noise, or something to be discounted in a dataset, may provide essential evidence for another.” (Owens, “Defining Data for Humanists”)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6b000a-5976-4328-bc56-fb20ae7e8778",
   "metadata": {},
   "source": [
    "Many packages have built-in stopwords lists, but you will probably need to modify these for the purposes of your analyses and create custom stopword lists. For a review of built-in stopwords list cf. Nothman, Qin and Yurchak. [\"Stop Word Lists in Free Open-source Software Packages\"](https://aclanthology.org/W18-2502/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d879400-6190-49e2-8ab9-e010a7a01f6b",
   "metadata": {},
   "source": [
    "**Adding to Built-in Stop Words List within the package**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52618903-e805-43ce-98f1-25bedc6056f4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Stopwords in spaCy\n",
    "import spacy\n",
    "\n",
    "#Download the language model you're interested in (this is the English pipeline)\n",
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc6dca4-7cdb-4455-9328-ccfd0f1c8a81",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Load language model and stopwords list\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "stopwords = nlp.Defaults.stop_words\n",
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847aa583-1f7b-4a74-8501-360322b6a1cc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add a word to spacy stopword list\n",
    "nlp.Defaults.stop_words.add('explain')\n",
    "stopwords = nlp.Defaults.stop_words\n",
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0240a0a6-5056-465b-a973-a35a84e047b9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Remove a word from spacy stopword list\n",
    "nlp.Defaults.stop_words.remove('explain')\n",
    "stopwords = nlp.Defaults.stop_words\n",
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da8f310-d8b6-415e-9038-679664f47164",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add multiple words to spacy stopword list\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "nlp.Defaults.stop_words |= {\"explain\",\"sample\"}\n",
    "stopwords = nlp.Defaults.stop_words\n",
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac01058-44ed-486b-9b2e-de93be776aa7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Remove multiple stopwords at once from spacy stopword list\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "nlp.Defaults.stop_words -= {'explain', 'sample'}\n",
    "stopwords = nlp.Defaults.stop_words\n",
    "stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159f1dbc-e003-439c-a961-2a7e546639e2",
   "metadata": {},
   "source": [
    "**Creating a Custom Stopword List**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bb8fb6-ddb1-4617-a7c6-d8e8e033ad71",
   "metadata": {},
   "source": [
    "Identifying what words to add to a stopwords list often rely on frequency counts in order to filter out frequent words that are not relevant to the analyses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cd4cb0-1abc-4d07-b0b3-4042daddc179",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Counting all the words in a text\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "#defining a tokenizing function which will split at and remove whitespace and punctuation\n",
    "#return words and numbers\n",
    "def tokenize(text):\n",
    "    lowercase_text = text.lower()\n",
    "    split_words = re.split('\\W+', lowercase_text)\n",
    "    return split_words\n",
    "\n",
    "#Read in text\n",
    "text = open('kafka_metamorphosis.txt', encoding=\"utf-8\").read()\n",
    "\n",
    "#Use our tokenizing function to tokenize the text\n",
    "all_the_words = tokenize(text)\n",
    "\n",
    "##Count frequencies of all the words\n",
    "all_the_words_count = Counter(all_the_words)\n",
    "all_the_words_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b59795-76ff-4ab1-ac30-e29736784c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Counting most frequent words in a text\n",
    "\n",
    "#How many most frequent words do you want to see?\n",
    "number_of_desired_words = 50\n",
    "\n",
    "#Return most frequent words\n",
    "most_frequent_all_the_words_count = all_the_words_count.most_common(number_of_desired_words)\n",
    "most_frequent_all_the_words_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50beaaf-307e-4a0e-aef5-d4fa77cf16ea",
   "metadata": {},
   "source": [
    "What words in this list are not relevant to your project? You can add them to your custom stopwords list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56369cc1-69a8-4e15-9848-3b40f7e5c2b9",
   "metadata": {},
   "source": [
    "**Building a custom stopwords list**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0b83b5-8a28-4efc-bab4-406115ad2d1b",
   "metadata": {},
   "source": [
    "It might be helpful to use an existing stopwords list as a starting point, and tailor that list to specific projects. For example, we could use the built-in spaCy stopwords list as a starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f298717-1529-4557-9405-0feba20b6d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stopwords in spaCy\n",
    "import spacy\n",
    "\n",
    "#Download the language model you're interested in (this is the English pipeline)\n",
    "#For french: fr_core_news_sm\n",
    "#For spanish: es_core_news_sm\n",
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9527579e-8120-41d9-861e-e461b6e25aa6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Load language model and stopwords list\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "stopwords = nlp.Defaults.stop_words\n",
    "sorted(list(stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e4b4a9-eceb-41e2-8e68-9e2bb9afab59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write out the spacy stopwords list to a txt file\n",
    "with open(\"spacy-stopwords-english.txt\", \"a\") as file_object:\n",
    "    for word in sorted(list(stopwords)): \n",
    "        file_object.write(word + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548c64aa-dfe2-48d3-ba3d-597d795ecc27",
   "metadata": {},
   "source": [
    "**Read in your stopwords list, use it in code, and add words to your stopwords list**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643135cb-e566-4677-9ac1-ef20f10e2372",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Open your txt file and convert to a Python list\n",
    "with open(\"spacy-stopwords-english.txt\", \"r\") as file_object:\n",
    "    custom_stopwords = [s.rstrip('\\n') for s in file_object.readlines()] \n",
    "\n",
    "custom_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961dcca1-5d96-44cf-b36f-c82b671a2e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Append a new work to the list\n",
    "custom_stopwords.append('got')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f01e42-902f-432d-92cd-45deb4f9ffb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Append multiple new words to the list\n",
    "custom_stopwords += ['gotten', 'mr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b916a66b-1720-423b-b9ed-09eaf671be14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove a word\n",
    "#Find the index of the word you want to remove\n",
    "index = custom_stopwords.index('gotten')\n",
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bf15e6-b7ac-437f-a806-dd7c754445fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#then delete word\n",
    "del custom_stopwords[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca35d1a-908d-4fe5-9c74-dea7baf1adc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write out the updated list and sort alphabetically\n",
    "with open(\"custom-stopwords.txt\", \"w\") as file_object:\n",
    "    for word in sorted(custom_stopwords):\n",
    "        file_object.write(word + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d624fd4d-ac58-4ae6-a6c4-e9ae02dd1864",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if a given word in list (True if in list, False if not in list)\n",
    "'friend' in custom_stopwords "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d400989-5249-4869-8c0d-9c1f3f024409",
   "metadata": {},
   "source": [
    "**Example of using your custom stopword list in code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2248a7b1-d870-4c6c-a0f9-b9e225944386",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Open your txt file and covert to a Python list\n",
    "with open(\"custom-stopwords.txt\", \"r\") as file_object:\n",
    "    custom_stopwords = [s.rstrip('\\n') for s in file_object.readlines()] \n",
    "\n",
    "custom_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af723353-7c1d-4033-9f3f-59f341835623",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "#Defining a tokenizing function\n",
    "def tokenize(text):\n",
    "    lowercase_text = text.lower()\n",
    "    split_words = re.split('\\W+', lowercase_text)\n",
    "    return split_words\n",
    "\n",
    "#Reading in text\n",
    "text = open('kafka_metamorphosis.txt', encoding=\"utf-8\").read()\n",
    "\n",
    "#Tokenizing text\n",
    "all_the_words = tokenize(text)\n",
    "\n",
    "#Filtering only the words not on stopwords list (you use your stopwords list variable here)\n",
    "meaningful_words = [word for word in all_the_words if word not in custom_stopwords]\n",
    "\n",
    "#Counting words\n",
    "meaningful_words_tally = Counter(meaningful_words)\n",
    "\n",
    "#How many frequent words we want to see\n",
    "number_of_desired_words = 50\n",
    "\n",
    "#Return most frequent words\n",
    "most_frequent_meaningful_words = meaningful_words_tally.most_common(number_of_desired_words)\n",
    "most_frequent_meaningful_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a41397c-59d7-4c73-a402-6c82c432fd96",
   "metadata": {},
   "source": [
    "****\n",
    "**Read in your stopwords list, use it in code, and add words to your stopwords list**\n",
    "\n",
    "This does the same thing as above but using pandas instead of Python lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95aa36a-4252-4258-a5d7-fbd1564b902d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#Read in stopwords list as pandas dataframe and convert to it to a list\n",
    "stopwords_df = pd.read_csv('spacy-stopwords-english.txt', names=['word'])\n",
    "custom_stopwords_list = stopwords_df['word'].to_list()\n",
    "custom_stopwords_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57710bb9-a285-4e69-ab31-47cfdbfb595f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding words to the list\n",
    "\n",
    "#Create list of words you want to add\n",
    "new_words = ['got', 'mr']\n",
    "\n",
    "#Create a dataframe of words you want to add\n",
    "new_words_df = pd.DataFrame(new_words, columns=['word'])\n",
    "new_words_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb9c3a2-2bc0-49d0-8c2b-c1b4c0e973a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenate/merge the old dataframe with new dataframe with new words in it, and sort it\n",
    "updated_stopwords_df = pd.concat([stopwords_df, new_words_df], ignore_index=True)\n",
    "updated_stopwords_df = updated_stopwords_df.sort_values(by='word')\n",
    "updated_stopwords_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c0f325-7cce-4eb7-9380-fd2d94d25b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if a word is in the list (False if not in list, True if in list)\n",
    "stopwords_df.word.str.contains('friend').any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16cf8f9-20de-4dbd-b0f2-3c5f57955ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write out the dataframe to a txt file\n",
    "updated_stopwords_df.to_csv('custom_stopwords.txt', sep=' ', header=None, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30ec2e4-baef-49df-9a3d-3a1c86ede0ab",
   "metadata": {},
   "source": [
    "**Example of using your custom stopword list in code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0c8a88-cd8d-43e7-bcd7-1d6b64b594fa",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#Read in stopwords list as pandas dataframe and convert to it to a list\n",
    "stopwords_df = pd.read_csv('custom-stopwords.txt', names=['word'])\n",
    "custom_stopwords_list = stopwords_df['word'].to_list()\n",
    "custom_stopwords_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741ea7b3-a82b-40d0-8f02-9dc8982d87a6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "#Defining a tokenizing function\n",
    "def tokenize(text):\n",
    "    lowercase_text = text.lower()\n",
    "    split_words = re.split('\\W+', lowercase_text)\n",
    "    return split_words\n",
    "\n",
    "#Reading in text\n",
    "text = open('kafka_metamorphosis.txt', encoding=\"utf-8\").read()\n",
    "\n",
    "#Tokenizing text\n",
    "all_the_words = tokenize(text)\n",
    "\n",
    "#Filtering only the words not on stopwords list (you use your stopwords list variable here)\n",
    "meaningful_words = [word for word in all_the_words if word not in custom_stopwords_list]\n",
    "\n",
    "#Counting words\n",
    "meaningful_words_tally = Counter(meaningful_words)\n",
    "\n",
    "#How many frequent words we want to see\n",
    "number_of_desired_words = 50\n",
    "\n",
    "#Return most frequent words\n",
    "most_frequent_meaningful_words = meaningful_words_tally.most_common(number_of_desired_words)\n",
    "most_frequent_meaningful_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d17ff0-56cc-46b1-bd44-bf2dfee98919",
   "metadata": {},
   "source": [
    "**Defining a function for removing a list of stopwords from a list of tokens**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c648ed-1b42-4fb9-aab1-e0afd4bed4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to filter out stopwords from your list of tokens\n",
    "def remove_stopwords(list_of_tokens, stopwords):\n",
    "    return [token for token in list_of_tokens if token not in stopwords]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7590de-84d5-491b-b5c0-39799e9e63d1",
   "metadata": {},
   "source": [
    "Acknowledgements: This notebook incorporates code and ideas from Melanie Walsh and Quinn Dombrowski's collaborations in Walsh's [\"Introduction to Cultural Analytics & Python](https://melaniewalsh.github.io/Intro-Cultural-Analytics/05-Text-Analysis/01-Multilingual-Text-Analysis.html). And from Jed Dobson's [notebooks](https://github.com/jeddobson/ENGL64.05-21F/blob/main/homework/Homework-03.ipynb) for his \"Cultural Analytics\" course at Dartmouth College."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
