{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3640caa8-3f28-4287-810e-1cf9045423f8",
   "metadata": {},
   "source": [
    "# Vector Space Models and Similarity Measures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7549d26b-3b45-468f-8505-3cb5a44ed0c6",
   "metadata": {},
   "source": [
    "## Introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe34be8-6c75-475e-804f-1c8fa19d1b5f",
   "metadata": {},
   "source": [
    "### Terminology: Vectorization, vectors, vector space\n",
    "\n",
    "**vectorization**: the processes by which text data become represented as vectors based on particular features (e.g. word frequencies, distinctive words, etc)\n",
    "\n",
    "> “_Vectorization_ refers to the process by which objects become represented as vectors; vectors stand in for the source objects. In the case of texts, vectorization involves the quantification of linguistic units and the values may include word frequencies, probability values, or co- occurrence relations, among other possible options.” (Dobson, James. “Vector Hermeneutics” in _Digital Scholarship in the Humanities_, Vol. 37, No. 1, 2022, p. 82)\n",
    "\n",
    "\n",
    "**vectors**: sequences of numbers used to identify a point or provide coordinates of language features in semantic space. When represented as vectors, semantic relationships can be manipulated and investigated in new ways.\n",
    "\n",
    "> “_Vectors_ should be understood as stored computable values that enable comparisons across a high-dimensional space of some number of texts.” (Dobson, James. “Vector Hermeneutics” in _Digital Scholarship in the Humanities_, Vol. 37, No. 1, 2022, p. 82)\n",
    "\n",
    "\n",
    "**vector space**: the high-dimensional geometric space defined by the set of vectors abstracted from the text data\n",
    "\n",
    "> “_Vector space_ is the large dimensional geometric space, the boundaries of which are defined by the included vectors, in which these objects exist.”\n",
    "(Dobson, James. “Vector Hermeneutics” in _Digital Scholarship in the Humanities_, Vol. 37, No. 1, 2022, p. 82)  \n",
    "\n",
    "\n",
    "**Vector space models**\n",
    "\n",
    "> “Vector models are produced through one-way transformations of text into another space.” (Dobson, James. “Vector Hermeneutics” in _Digital Scholarship in the Humanities_, Vol. 37, No. 1, 2022, p. 82)\n",
    "\n",
    "> “Vector space models are representations of text in that they provide an alternate mode of access to some aspects of the information contained within text.” (Dobson, James. “Vector Hermeneutics” in _Digital Scholarship in the Humanities_, Vol. 37, No. 1, 2022, p. 82)\n",
    "\n",
    "> “[Vector models involve the] numerical representation of information extracted from textual sources.” (Dobson, James. “Vector Hermeneutics” in _Digital Scholarship in the Humanities_, Vol. 37, No. 1, 2022, p. 84)\n",
    "\n",
    "Creating word vectors from text data involves a process in which distributions of vocabulary across bodies of text become represented in mathematical space. This allows us to imagine semantic relations as spatial relations and to use linear algebra to investigate and manipulate semantic relations. This can help us reconstruct and explore conceptual relationships in cultural conceptual analysis in new ways.\n",
    "\n",
    "What does this mean? Let’s look at some examples below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a75eb7-cb2c-4641-b8e4-e76dbc48aba2",
   "metadata": {},
   "source": [
    "### Creating word vectors\n",
    "\n",
    "Imagine we have a fragment of text: “It was the best of times, it was the worst of times.”  \n",
    "\n",
    "Let’s map out how these words relate to one another by creating a spreadsheet for which each row is one (unique) word in the sentence and each column is the context of that word (in this case, context means the word immediately before and after it). The values in each cell correspond with how many times the word occurs in the given context. The numbers in the rows constitute that word's vector. Because there are ten possible contexts, this is a ten dimensional space.  \n",
    "\n",
    "![dtm-best-worst](dtm-best-worst.png)  \n",
    "\n",
    "The rows are the vectors of each word. The vector for `of = [0, 0, 0, 0, 1, 0, 0, 0, 1, 0]`  \n",
    "\n",
    "You can see that `best` and `worst` have the same vector, the same set of coordinates, because they appear in exactly the same contexts (between `the` and `of`). \n",
    "\n",
    "This means that `best` and `worst` have similar semantic functions. The model is capturing something about how language behaves in this sentence — even though `best` and `worst` have opposite meanings, Dickens is precisely playing on paradox by setting up parallels between oppositions in this sentence. Words with opposite meanings are given the same place and function in the sentences in order to convey the sense of crisis and confusion in late 19th century Britain.\n",
    "\n",
    "**Document-Term-Matrix**  \n",
    "The example above involves a single sentence. Typically, you are working with large numbers of text, but the process of vectorization is similar.  In creating a Document-Term-Matrix (one way of creating word vectors), you create a matrix (table) where each row stands for a document, and each column is a term in the vocabulary of the corpus. Each row represents a vector of the distribution of vocabulary in document \n",
    "\n",
    "There are many different ways of creating word vectors involving a vast array of different parameters.\n",
    "\n",
    "\n",
    "**Neural Language Models**  \n",
    "word2vec implementations learn from a given training data the repetitions of words in similar context windows and identify semantic relationships between words by observing them in context. Each vector the record of repeated language features.\n",
    "\n",
    "> “the word2vec embedding model, is also trained on texts but its vector space registers multiple aspects of language as found in the modeled texts. We might best think of this class of vectors as inscribed traces through their textual sources. The shape of these traces record repeated patterns of language use. Each trace, then, itself is a record, a line of inquiry and connection running through, constructing, and calling into being a semantic universe.” (Dobson, James. “Vector Hermeneutics” in _Digital Scholarship in the Humanities_, Vol. 37, No. 1, 2022, p. 91)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2082a1e9-2f1d-46cc-9524-fa7cf4a86043",
   "metadata": {},
   "source": [
    "### Exploring semantic relations with word vectors: a playful example with color words\n",
    "\n",
    "Let’s look at an example by playing with vectors for color words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8817b667-79e4-4510-903a-169279d4851d",
   "metadata": {},
   "source": [
    "Instead of vectorizing by training on text data, imagine we have a system of color where each color is associated with a particular code point. Similar colors have similar code points. We'll be working with this color data from the [xkcd color survey](https://github.com/dariusk/corpora/blob/master/data/colors/xkcd.json). The data links a color code with a name of a color (determined by groups of participants).  [Here’s a page](https://xkcd.com/color/rgb/) that shows the colors and their codes. We'll turn this color code into a system of coordinates (i.e. vectors) and then manipulate the coordinates to explore relations between these colors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9b1c9a-ed5b-46ee-a9e3-9accfb50f6cc",
   "metadata": {},
   "source": [
    "Note: the code here is for illustrative purposes only, it will be too slow for larger vector space models. Don't get too bogged down in the details of the code, this is for illustrating what creating and manipulating word vectors can do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c07e8b-707a-4f97-95ab-a51e5b6fd4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3de7cd-a7d5-42f5-b39f-81c4e18cc2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "color_data = json.loads(open(\"xkcd.json\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae3f003-1b43-43e5-9ca0-f2cebd8bea0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The following function converts colors \n",
    "#from hex format (#1a2b3c) to a tuple of integers:\n",
    "def hex_to_int(s):\n",
    "    s = s.lstrip(\"#\")\n",
    "    return int(s[:2], 16), int(s[2:4], 16), int(s[4:6], 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636e095d-3690-4538-9356-8ac2c479e75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a dictionary and populate it with mappings from \n",
    "#color names to RGB vectors for each color in the data:\n",
    "colors = dict()\n",
    "for item in color_data['colors']:\n",
    "    colors[item['color']] = hex_to_int(item['hex'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85f1ac1-86df-461d-898b-cda7c6300fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors['olive']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6972d535-491b-45f9-b564-3b6ceb8e0bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors['black']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8ae1c4-4559-45a9-ac77-ea89849a16b5",
   "metadata": {},
   "source": [
    "The cell below defines a function to find the closest item to an arbitrary vector by finding the distance between the target vector and each item in the space, in turn, then sorting the list from closest to farthest. The closest() function below does just that. By default, it returns a list of the ten closest items to the given vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da330f06-1246-4cc2-94ed-3f3af6c1ab2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def distance(coord1, coord2): # returns the Euclidean distance between two points\n",
    "    return math.sqrt(sum([(i - j)**2 for i, j in zip(coord1, coord2)]))\n",
    "\n",
    "def closest(space, coord, n=10):\n",
    "    closest = []\n",
    "    for key in sorted(space.keys(),\n",
    "                        key=lambda x: distance(coord, space[x]))[:n]:\n",
    "        closest.append(key)\n",
    "    return closest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982db709-7e3a-44e7-b54d-4ae33b9d62aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "closest(colors, colors['red'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c88db5-3f62-4aa3-b495-caedac3449b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining a function to subtract two vectors from one another\n",
    "import math\n",
    "def subtractv(coord1, coord2):\n",
    "    return [c1 - c2 for c1, c2 in zip(coord1, coord2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840f3bb2-0ffd-4439-9336-9abf9f557ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining a function to add two vectors to one another\n",
    "def addv(coord1, coord2):\n",
    "    return [c1 + c2 for c1, c2 in zip(coord1, coord2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae5b234-5ebe-4832-ad31-4c39f6d6f131",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test: \n",
    "#the distance from \"red\" to \"green\" is greater than the distance from \"red\" to \"pink\"\n",
    "distance(colors['red'], colors['green']) > distance(colors['red'], colors['pink'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22f5f1a-eaae-42ea-9389-ab16858a814b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding closest vectors to vector \"purple\"\n",
    "closest(colors, colors['purple'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87a6438-7409-4147-8631-23899ba20c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding the word vectors closest to the vector resulting from \n",
    "#subtracting \"red\" from \"purple\"\n",
    "closest(colors, subtractv(colors['purple'], colors['red']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff14390e-0763-4bf6-a9e5-4002925cf24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding the word vectors closest to the vector resulting from \n",
    "#adding \"black\" and \"\"\n",
    "closest(colors, addv(colors['white'], colors['black']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb28e21-fde3-4177-94a8-a3b0ed52b20d",
   "metadata": {},
   "source": [
    "#### Vectors and Comparison — Computing relations between vectors\n",
    "\n",
    "Comparison involves investigating the relations between different elements — the uniqueness, sameness, distinctions, overlaps between elements  relative to other elements.\n",
    "\n",
    "Word vectors allow for a number of different methods for calculating relations similarity and difference (based on properties abstracted from text into vector representation). When distributions of words have been rendered as points in space, we can ask questions such as: How far apart or close together are these two points? \n",
    "\n",
    "\n",
    "> “Vectors are defined more by the operations performed on them, the vector-wise numerical computations and comparisons, than their particular data structures and formats. While a single indexed object comprised some indeterminate sequence of values might be considered as a vector, it is the computational manipulation of multiple objects grasped in their entirety that defines a vector.”  (Dobson, James. “Vector Hermeneutics” in _Digital Scholarship in the Humanities_, Vol. 37, No. 1m 2022, p. 82)\n",
    "\n",
    "> “all words or text fragments are placed in relation to each other. It is an instrument through which we can render language comparable but this comparison always takes place through the frame inscribed by the operator.” (Dobson, James. “Vector Hermeneutics” in _Digital Scholarship in the Humanities_, Vol. 37, No. 1m 2022, p. 88)\n",
    "\n",
    "> “Measuring pair-wise distance, the distance in vector space between two individual word vectors, even at two hundred dimensions, is a trivial task but we must choose the appropriate method by which we determine the distance, our distance metric. Do we use cosine similarity and determine the angle corresponding to our two selected vectors? Or do we use Euclidean distance and measure the shortest path through vector space? When comparing multiple vectors for visualization in two or three dimensions we might choose principal components analysis or t-distributed Stochastic Neighbor Embedding to reduce the dimensionality of our data. These methods all produce alternate representations of vector space that introduce distortions and give greater significance to some aspects of data. They do not alter nor do they rewrite the vector space model but they do create a new representation of this space. The initial model and its geometry remain in- tact, but the space has been selectively extracted, extended, or warped to form a new representational object.” (Dobson, James. “Vector Hermeneutics” in _Digital Scholarship in the Humanities_, Vol. 37, No. 1m 2022, p. 88)\n",
    "\n",
    "\n",
    "\n",
    "#### Cosine similarity \n",
    "\n",
    "We can measure closeness and distance between vectors in vector space using distance metrics. Distance metrics allow us to investigate computationally about how alike or different texts or concepts may be. With distance metrics we can take word-based features of a set of documents and measures the similarity among documents based on their distance from one another in vector space. \n",
    "\n",
    "According to the distributional hypothesis: the closer the words in space > the more similar text contexts of use > the more similar their meanings and their semantic functions.\n",
    "\n",
    "> “Similarity in vector space, where similarity means closeness as measured with one of the above-mentioned distance metrics within the model-defined geometrical space, has been assumed to suggest something about the similarity of the meaning of the words, or at least their usage patterns in supplied training data.” (Dobson, James. “Vector Hermeneutics” in _Digital Scholarship in the Humanities_, Vol. 37, No. 1m 2022, p. 89)\n",
    "\n",
    "\n",
    "Cosine similarity is measured by the cosine of the angle between two points. This returns some number between 0 and 1 — the closer to 1, the closer the angle, the more similar the elements are.\n",
    "\n",
    "Because cosine similarity describes the angle between two points rather than their exact placement in space. This means that cosine distance is much less effected by magnitude, or how large your numbers are. No matter what the lengths of the vectors are, the angle still tells us how close these two vectors look. This is really useful because it means that we can directly compare short and long bodies of text.\n",
    "\n",
    "We can therefore use cosine similarity to identify concepts that share similar semantic spaces or functions. \n",
    "\n",
    "\n",
    "### Vector arithmetic\n",
    "\n",
    "We can also apply arithmetic to vectors to manipulate the conceptual space these vectors represent. \n",
    "\n",
    "Adding vectors together will bring to light the shared semantic spaces these words occupy. \n",
    "\n",
    "Subtracting vectors from one another will leave behind what is unique to the semantic space of a vectors as distinct from the subtracted vector. For example, word vectors may encode the polysemy, the many different meanings of a word that are simultaneously present in a word. \n",
    "For example, bank may refer to a bank or to a river bank. We can subtract river from bank which will remove the shared semantic space between these two vectors and leave us with the semantic space that bank occupies as distinct from river. \n",
    "\n",
    "These manipulation of vectors allow for radically new ways of investigating semantic relations between concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983b3861-0d0e-492f-80f5-e48d7a2e8788",
   "metadata": {},
   "source": [
    "## Document-term-matrix and Cosine similarity between documents using Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ddb05f-8466-40f8-846f-909b552a0afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abb259b-17b5-4fa1-8c71-18297429d126",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The files we want to vectorize\n",
    "input_files = glob.glob('kafka-corpus/*')\n",
    "print(input_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cbc3c7-eb84-4b3b-b66c-13150633e716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting our texts into a document-term matrix.\n",
    "\"\"\"Parameters: \n",
    "1-Set input to \"filename\" to tell CountVectorizer to accept a list of filenames to open and read.\n",
    "3-Set max_df to 0.7. DF stands for document frequency.\n",
    "This parameter tells CountVectorizer that you’d like to eliminate words that appear in more than 70% \n",
    "of the documents in the corpus. This setting will eliminate the most common words \n",
    "(articles, pronouns, prepositions, etc.) without the need for a stop words list.\n",
    "\"\"\"\n",
    "#Try modifying different parameters.\n",
    "#cf. help(vectorizer) for more information\n",
    "\n",
    "vectorizer = CountVectorizer(input='filename',\n",
    "                             lowercase=True,\n",
    "                             strip_accents='unicode',\n",
    "                             max_df=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd03587-05aa-42a5-96af-3ab94485c3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This does the actual vectorization and creates a document term matrix\n",
    "dtm = vectorizer.fit_transform(input_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c7506d-257b-448c-8c68-ce839ea5abba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return total number of documents and the number of items in the vocabulary\n",
    "dc, vc = dtm.shape\n",
    "print('document count:',dc,'vocabulary count:',vc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edbaf4b-befb-4036-950d-e1146295f016",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Inspect vocabulary items\n",
    "vectorizer.vocabulary_.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682508dd-7ea3-47da-85ab-25a556556f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are our top words across all documents?\n",
    "vocab_sums = dtm.sum(axis=0)\n",
    "sorted_vocab = [(v, vocab_sums[0, i]) for v, i in vectorizer.vocabulary_.items()]\n",
    "sorted_vocab = sorted(sorted_vocab, key = lambda x: x[1], reverse=True)\n",
    "\n",
    "# display top twenty words\n",
    "for i in range(1,20):\n",
    "    print(sorted_vocab[i][0],\"->\",sorted_vocab[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216290ac-6e3c-4754-8b76-4073532a5cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a distance matrix using Cosine similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cosine_dist_matrix = 1 - cosine_similarity(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86d6d52-7195-40c7-a20e-55579426b002",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assessing similarties between texts based on term frequencies across documents\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "#Creates a dataframe with cosine distances between the texts\n",
    "#calculated from vectors of word counts for each text\n",
    "cosine_distances = pd.DataFrame(squareform(pdist(dtm.toarray(), metric='cosine')), \n",
    "                                columns=input_files, index=input_files)\n",
    "cosine_distances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba949ccc-ebf0-4059-9e31-764619bda48b",
   "metadata": {},
   "source": [
    "## Word vectors using word2vec and gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0f4ed6-ccf4-4df8-9754-b9e834f35f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940d41e3-1653-44b4-8918-743ba59cc641",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging all texts into a single list\n",
    "from pathlib import Path\n",
    "directory_path = 'kafka-corpus'\n",
    "all_docs = []\n",
    "\n",
    "for filepath in Path(directory_path).glob(\"*.txt\"):\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "        all_docs.append(text)\n",
    "\n",
    "len(all_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11691f7-2ae2-4df9-a6ef-ae752f268146",
   "metadata": {},
   "source": [
    "Gensim takes sentences as its input. So let's define a function that a takes a list of texts (e.g. our all_docs list) and converts it into sentences for gensim word2vec to use. The function will lower-case text and tokenize by sentence and word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd0e81a-e710-4f05-947a-82ac074c0002",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "def make_sentences(list_txt):\n",
    "    all_txt = []\n",
    "    counter = 0\n",
    "    for txt in list_txt:\n",
    "        lower_txt = txt.lower()\n",
    "        sentences = sent_tokenize(lower_txt)\n",
    "        sentences = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "        all_txt += sentences\n",
    "        counter += 1\n",
    "    return all_txt\n",
    "\n",
    "sentences = make_sentences(all_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e8a055-8b6b-4ea1-a60b-e99389946b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traning the models\n",
    "# Try playing around with vector_size and min_count to see how that affects the models\n",
    "kafka_model = gensim.models.Word2Vec(\n",
    "    sentences,\n",
    "    min_count=5, # default is 5; ignores all words with total frequency lower than 5 \n",
    "    vector_size=50) # size of NN layers; default is 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bced224-4e92-4f0e-a260-2571592105e2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Inspect the vocabulary\n",
    "kafka_model.wv.key_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be85fca-520a-49c7-9554-01920d85f87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find nearest word vectors by cosine distance\n",
    "kafka_model.wv.most_similar('gregor', topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03baee74-fc38-4880-8530-74e91279890c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find cosine distance between two given word vectors, the closer to 1 the more similar\n",
    "print(kafka_model.wv.similarity(w1='disgust',w2='food'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46df0a2d-b6aa-4d22-be3f-03c730095dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subtract two vectors\n",
    "#Remove the sense of \"life\" from \"hopeless\" and see \n",
    "kafka_model.wv.most_similar(positive=['hopeless'], negative=['life'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bafa2c-b31d-4f82-b42c-09f0952fbd49",
   "metadata": {},
   "source": [
    "_Acknowledgements_: This notebook is inspired by Allison Parrish’s [“Understanding word vectors” notebook](https://gist.github.com/aparrish/2f562e3737544cf29aaf1af30362f469), Dan Sinykin's [\"Word Embedding Models\" notebook](https://github.com/sinykin/QTM-340/blob/master/notebooks/class13-word-vectors-complete-ds.ipynb), Teddy Roland's [\"Word2Vec\" notebook](https://github.com/teddyroland/BBB-Word2Vec/blob/master/Word2Vec.ipynb), and John Ladd's [\"Understanding and Using Common Similarity Measures for Text Analysis\"](https://programminghistorian.org/en/lessons/common-similarity-measures)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
