{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82c49f88-87b3-4bae-ada6-711bcaaf13be",
   "metadata": {},
   "source": [
    "# Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8aaa0fb-e84f-4ee9-9722-a04bcfc41cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction import _stop_words as stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d617f514-1d8b-44eb-b4ed-52c6e44a0186",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in file and split into segments to vectorize\n",
    "text = open('kafka_metamorphosis.txt', encoding=\"utf-8\").read()\n",
    "\n",
    "def tokenize(text):\n",
    "    lowercase_text = text.lower()\n",
    "    split_words = re.split('\\W+', lowercase_text)\n",
    "    return split_words\n",
    "\n",
    "tokens = tokenize(text)\n",
    "\n",
    "#Split into a collection of documents of 1,000 words\n",
    "segment_length = 1000\n",
    "data_to_vectorize = list()\n",
    "\n",
    "nseg = round(len(tokens) / segment_length)\n",
    "for i in range(nseg):\n",
    "    segment = tokens[segment_length*i:segment_length*(i+1)]\n",
    "    data_to_vectorize.append(' '.join(segment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef54d5b7-b45b-4aac-969f-0a7821b1df90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup vectorizer and process text\n",
    "vec = CountVectorizer(input='content',\n",
    "                      stop_words='english',\n",
    "                      lowercase=True)\n",
    "\n",
    "dtm = vec.fit_transform(tokens)\n",
    "dc, vc = dtm.shape\n",
    "print(\"read {0} documents with {1} vocabulary\".format(dc,vc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbe6d5c-ae23-4a55-884f-e5647d9026bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the LDA model\n",
    "\n",
    "#Parameter n_components = number of topics to extract (if topics are too similar, extract more)\n",
    "simple_lda_model = LatentDirichletAllocation(n_components=10,\n",
    "                                             max_iter=5,\n",
    "                                             learning_method='batch',\n",
    "                                             random_state=1)    \n",
    "# get fitted data and transformed matrix\n",
    "simple_lda_data = simple_lda_model.fit(dtm)\n",
    "\n",
    "# extract the features to a simple list\n",
    "feature_names = vec.get_feature_names_out()\n",
    "\n",
    "#Variable n_words: how many words do we want for each topic?\n",
    "n_words = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1727af5-6abe-4b18-880e-35f52aae5c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate list of topics\n",
    "for topic_idx, topic in enumerate(simple_lda_model.components_):\n",
    "    print(\"Topic #%d:\" % topic_idx)\n",
    "    for i in topic.argsort()[:-n_words - 1:-1]:\n",
    "        print(\"{0} \".format(feature_names[i]),end=\"\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f589a98-f5f3-47e8-8f9f-938e4267c44b",
   "metadata": {},
   "source": [
    "_Acknowledgements_: This notebook is heavily inspired by Jed Dobson's [\"Cultural Analytics\" course notebook](https://github.com/jeddobson/ENGL64.05-21F/blob/main/homework/Homework-03.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
