{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "847ead97-97d3-4997-81a3-377891bffb58",
   "metadata": {},
   "source": [
    "# Analyzing keywords with Term Frequency-Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdfbe05-d4e0-4d2a-b4b4-582a124b00cc",
   "metadata": {},
   "source": [
    "**Contents**  \n",
    "[Tf-idf Overview](#section-1)  \n",
    "\n",
    "[Digging Deeper into Tf-idf](#section-2)\n",
    "> [Stopwords](#section-3)  \n",
    "\n",
    "> [A note on tokenizing and preprocessing](#section-4)  \n",
    "\n",
    "> [Playing with Tf-idf parameters](#section-5)\n",
    "\n",
    "[Interpreting Tf-idf outputs](#section-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1bf48a-7dcf-4853-95c8-c7aed309be08",
   "metadata": {},
   "source": [
    "<a id='section-1'></a>\n",
    "# Tf-idf Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee3d94c-bde6-49ee-ae8e-b298e47edd1c",
   "metadata": {},
   "source": [
    "Keyness or distinctiveness is a catchall term for a constellation of statistical measures that attempt to indicate the numerical significance of a term to a document or set of documents, in direct comparison with a larger set of documents or corpus. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead1300d-cbac-4b45-a3a5-dd96d88eba90",
   "metadata": {},
   "source": [
    "With TF-IDF each term is weighted by dividing the term frequency by the number of documents in the corpus containing the word. It gives weight to terms that appear in a document but are rare or absent in other documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd064dc-d224-4a33-98b5-cf644e75dfc0",
   "metadata": {},
   "source": [
    "TF-IDF is calculated by taking the number of times a term occurs in a document (term frequency). Then taking the number of documents in which the same term occurs at least once divided by the total number of documents (document frequency), and that fraction is flipped on its head (inverse document frequency =  log((1 + total_number_of_documents) / (number_of_documents_with_term +1)) + 1). Then you multiply the two numbers together (term_frequency * inverse_document_frequency). The reason we take the inverse, or flipped fraction, of document frequency is to boost the rarer words that occur in relatively few documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092b1cdb-520f-49bd-8fa5-a4f711ed00c1",
   "metadata": {},
   "source": [
    "In this notebook we use the implemention of tf-idf in Scikit-learn (sklearn). Let's first run through using tf-idf in sklearn to get a sense of what it does and then we'll go through it again whilst paying more attention to parameters and stopwords (\"Digging Deeper into Tf-idf\" section)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289fd841-5d57-480a-af89-4cdebb65391c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 600)\n",
    "from pathlib import Path  \n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1980581-ceea-4290-8f6d-efa2569d78bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data: set up path to files and a variable with file names\n",
    "directory_path = 'kafka-corpus/'\n",
    "text_files = glob.glob(f'{directory_path}/*.txt')\n",
    "text_titles = [Path(text).stem for text in text_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1d3429-5462-4e9a-83cd-953b95eaca0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up tf-idf vectorizing (we'll go over parameters in more detail further below)\n",
    "tfidf_vectorizer = TfidfVectorizer(input='filename', stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c42c8b-2937-40c3-8aff-e24d7bbe22c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Actually do the vectorizing\n",
    "tfidf_vector = tfidf_vectorizer.fit_transform(text_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360f80cf-95f8-4cc6-834b-de751332562b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a DataFrame out of the resulting tf–idf vectors, \n",
    "#setting the “feature names” (words in vocabulary) as columns and the titles as rows\n",
    "tfidf_df = pd.DataFrame(tfidf_vector.toarray(), \n",
    "                        index=text_titles, \n",
    "                        columns=tfidf_vectorizer.get_feature_names_out())\n",
    "tfidf_df.head(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76861142-e2af-4793-9e1a-9e0f3f2ee2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add row for Document Tf-idf (sum of tf-idf scores for each word across all documents)\n",
    "tfidf_df.loc['00_Document Tf-idf'] = tfidf_df.sum(axis=0)\n",
    "tfidf_df.head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b347064f-2f86-4229-b89a-3c8bfb638df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Re-organize so words are in rows rather than columns\n",
    "tfidf_df = tfidf_df.sort_index()\n",
    "stacked_tfidf_df = tfidf_df.stack().reset_index()\n",
    "stacked_tfidf_df = stacked_tfidf_df.rename(columns={0:'tfidf', 'level_0': 'document','level_1': 'term', 'level_2': 'term'})\n",
    "stacked_tfidf_df.sample(n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37661688-ac58-452d-91ef-781bba68bc75",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Top 10 words with the highest tf–idf for every story\n",
    "top_tfidf = stacked_tfidf_df.sort_values(by=['document','tfidf'], ascending=[True,False]).groupby(['document']).head(10)\n",
    "top_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7640217-d5a3-4fab-8c45-79e34727c72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Zoom in on particular words\n",
    "#What documents have the given word in their top significant words?\n",
    "top_tfidf[top_tfidf['term'].str.contains('people')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e45934e-6553-460c-a6d4-cfae3bec11c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display signficance scores of the given word across all documents\n",
    "stacked_tfidf_df[stacked_tfidf_df['term'].str.contains('people')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07654272-4d37-4417-b0b8-065a131e4255",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Zoom in on particular document\n",
    "#What are the top significant words for a given document?\n",
    "top_tfidf[top_tfidf['document'].str.contains('kafka_a-hunger-artist')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c225ec8e-ab9b-4ee5-b74e-cf67c0c418d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#What are the top 20 significant words for the given document?\n",
    "(stacked_tfidf_df[stacked_tfidf_df['document']\n",
    "                  .str.contains('kafka_a-hunger-artist')]\n",
    " .sort_values('tfidf', ascending=False)\n",
    " .head(20)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756b887c-46a4-4c92-8419-05f3b8c5e0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create bar plots of top 10 significant words for all documents, and each story\n",
    "import seaborn as sns\n",
    "sns.catplot(data=top_tfidf, row='document', x='tfidf', y='term', kind='bar', sharey=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061b27a6-41a5-417d-b49f-d93682f96f0e",
   "metadata": {},
   "source": [
    "<a id='section-2'></a>\n",
    "## Digging Deeper into Tf-idf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efa6237-760b-43d5-8fac-d070f88eb580",
   "metadata": {},
   "source": [
    "<a id='section-3'></a>\n",
    "### Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9eb0e8e-faf9-4882-a220-c1cbcad30729",
   "metadata": {},
   "source": [
    "You might already have noticed that there are some words you might want to remove from the analyses in order to create more relevant results. Above we used the sci-kit learn’s built-in stopwords list. We've discussed how these lists might be problematic and might need to be modified in order to be more relevant for our particular research aims."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f97fe4-0fe5-4f71-9d58-b685568e7f1f",
   "metadata": {},
   "source": [
    "First we need to build our custom stopwords list. We might start by looking at [Scikit Learn’s built-stopwords list](https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/feature_extraction/_stop_words.py) (also in cell below) in order to identify some terms we want or don't want on our list. Maybe you’ve already identified some words from the previous analyses, add those too. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44982e02-272d-411b-bb81-681ae52e82b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_stopwords = (\n",
    "    [\n",
    "        \"a\",\n",
    "        \"about\",\n",
    "        \"above\",\n",
    "        \"across\",\n",
    "        \"after\",\n",
    "        \"afterwards\",\n",
    "        \"again\",\n",
    "        \"against\",\n",
    "        \"all\",\n",
    "        \"almost\",\n",
    "        \"alone\",\n",
    "        \"along\",\n",
    "        \"already\",\n",
    "        \"also\",\n",
    "        \"although\",\n",
    "        \"always\",\n",
    "        \"am\",\n",
    "        \"among\",\n",
    "        \"amongst\",\n",
    "        \"amoungst\",\n",
    "        \"amount\",\n",
    "        \"an\",\n",
    "        \"and\",\n",
    "        \"another\",\n",
    "        \"any\",\n",
    "        \"anyhow\",\n",
    "        \"anyone\",\n",
    "        \"anything\",\n",
    "        \"anyway\",\n",
    "        \"anywhere\",\n",
    "        \"are\",\n",
    "        \"around\",\n",
    "        \"as\",\n",
    "        \"at\",\n",
    "        \"back\",\n",
    "        \"be\",\n",
    "        \"became\",\n",
    "        \"because\",\n",
    "        \"become\",\n",
    "        \"becomes\",\n",
    "        \"becoming\",\n",
    "        \"been\",\n",
    "        \"before\",\n",
    "        \"beforehand\",\n",
    "        \"behind\",\n",
    "        \"being\",\n",
    "        \"below\",\n",
    "        \"beside\",\n",
    "        \"besides\",\n",
    "        \"between\",\n",
    "        \"beyond\",\n",
    "        \"bill\",\n",
    "        \"both\",\n",
    "        \"bottom\",\n",
    "        \"but\",\n",
    "        \"by\",\n",
    "        \"call\",\n",
    "        \"can\",\n",
    "        \"cannot\",\n",
    "        \"cant\",\n",
    "        \"co\",\n",
    "        \"con\",\n",
    "        \"could\",\n",
    "        \"couldnt\",\n",
    "        \"cry\",\n",
    "        \"de\",\n",
    "        \"describe\",\n",
    "        \"detail\",\n",
    "        \"do\",\n",
    "        \"done\",\n",
    "        \"down\",\n",
    "        \"due\",\n",
    "        \"during\",\n",
    "        \"each\",\n",
    "        \"eg\",\n",
    "        \"eight\",\n",
    "        \"either\",\n",
    "        \"eleven\",\n",
    "        \"else\",\n",
    "        \"elsewhere\",\n",
    "        \"empty\",\n",
    "        \"enough\",\n",
    "        \"etc\",\n",
    "        \"even\",\n",
    "        \"ever\",\n",
    "        \"every\",\n",
    "        \"everyone\",\n",
    "        \"everything\",\n",
    "        \"everywhere\",\n",
    "        \"except\",\n",
    "        \"few\",\n",
    "        \"fifteen\",\n",
    "        \"fifty\",\n",
    "        \"fill\",\n",
    "        \"find\",\n",
    "        \"fire\",\n",
    "        \"first\",\n",
    "        \"five\",\n",
    "        \"for\",\n",
    "        \"former\",\n",
    "        \"formerly\",\n",
    "        \"forty\",\n",
    "        \"found\",\n",
    "        \"four\",\n",
    "        \"from\",\n",
    "        \"front\",\n",
    "        \"full\",\n",
    "        \"further\",\n",
    "        \"get\",\n",
    "        \"give\",\n",
    "        \"go\",\n",
    "        \"had\",\n",
    "        \"has\",\n",
    "        \"hasnt\",\n",
    "        \"have\",\n",
    "        \"he\",\n",
    "        \"hence\",\n",
    "        \"her\",\n",
    "        \"here\",\n",
    "        \"hereafter\",\n",
    "        \"hereby\",\n",
    "        \"herein\",\n",
    "        \"hereupon\",\n",
    "        \"hers\",\n",
    "        \"herself\",\n",
    "        \"him\",\n",
    "        \"himself\",\n",
    "        \"his\",\n",
    "        \"how\",\n",
    "        \"however\",\n",
    "        \"hundred\",\n",
    "        \"i\",\n",
    "        \"ie\",\n",
    "        \"if\",\n",
    "        \"in\",\n",
    "        \"inc\",\n",
    "        \"indeed\",\n",
    "        \"interest\",\n",
    "        \"into\",\n",
    "        \"is\",\n",
    "        \"it\",\n",
    "        \"its\",\n",
    "        \"itself\",\n",
    "        \"keep\",\n",
    "        \"last\",\n",
    "        \"latter\",\n",
    "        \"latterly\",\n",
    "        \"least\",\n",
    "        \"less\",\n",
    "        \"ltd\",\n",
    "        \"made\",\n",
    "        \"many\",\n",
    "        \"may\",\n",
    "        \"me\",\n",
    "        \"meanwhile\",\n",
    "        \"might\",\n",
    "        \"mill\",\n",
    "        \"mine\",\n",
    "        \"more\",\n",
    "        \"moreover\",\n",
    "        \"most\",\n",
    "        \"mostly\",\n",
    "        \"move\",\n",
    "        \"much\",\n",
    "        \"must\",\n",
    "        \"my\",\n",
    "        \"myself\",\n",
    "        \"name\",\n",
    "        \"namely\",\n",
    "        \"neither\",\n",
    "        \"never\",\n",
    "        \"nevertheless\",\n",
    "        \"next\",\n",
    "        \"nine\",\n",
    "        \"no\",\n",
    "        \"nobody\",\n",
    "        \"none\",\n",
    "        \"noone\",\n",
    "        \"nor\",\n",
    "        \"not\",\n",
    "        \"nothing\",\n",
    "        \"now\",\n",
    "        \"nowhere\",\n",
    "        \"of\",\n",
    "        \"off\",\n",
    "        \"often\",\n",
    "        \"on\",\n",
    "        \"once\",\n",
    "        \"one\",\n",
    "        \"only\",\n",
    "        \"onto\",\n",
    "        \"or\",\n",
    "        \"other\",\n",
    "        \"others\",\n",
    "        \"otherwise\",\n",
    "        \"our\",\n",
    "        \"ours\",\n",
    "        \"ourselves\",\n",
    "        \"out\",\n",
    "        \"over\",\n",
    "        \"own\",\n",
    "        \"part\",\n",
    "        \"per\",\n",
    "        \"perhaps\",\n",
    "        \"please\",\n",
    "        \"put\",\n",
    "        \"rather\",\n",
    "        \"re\",\n",
    "        \"same\",\n",
    "        \"see\",\n",
    "        \"seem\",\n",
    "        \"seemed\",\n",
    "        \"seeming\",\n",
    "        \"seems\",\n",
    "        \"serious\",\n",
    "        \"several\",\n",
    "        \"she\",\n",
    "        \"should\",\n",
    "        \"show\",\n",
    "        \"side\",\n",
    "        \"since\",\n",
    "        \"sincere\",\n",
    "        \"six\",\n",
    "        \"sixty\",\n",
    "        \"so\",\n",
    "        \"some\",\n",
    "        \"somehow\",\n",
    "        \"someone\",\n",
    "        \"something\",\n",
    "        \"sometime\",\n",
    "        \"sometimes\",\n",
    "        \"somewhere\",\n",
    "        \"still\",\n",
    "        \"such\",\n",
    "        \"system\",\n",
    "        \"take\",\n",
    "        \"ten\",\n",
    "        \"than\",\n",
    "        \"that\",\n",
    "        \"the\",\n",
    "        \"their\",\n",
    "        \"them\",\n",
    "        \"themselves\",\n",
    "        \"then\",\n",
    "        \"thence\",\n",
    "        \"there\",\n",
    "        \"thereafter\",\n",
    "        \"thereby\",\n",
    "        \"therefore\",\n",
    "        \"therein\",\n",
    "        \"thereupon\",\n",
    "        \"these\",\n",
    "        \"they\",\n",
    "        \"thick\",\n",
    "        \"thin\",\n",
    "        \"third\",\n",
    "        \"this\",\n",
    "        \"those\",\n",
    "        \"though\",\n",
    "        \"three\",\n",
    "        \"through\",\n",
    "        \"throughout\",\n",
    "        \"thru\",\n",
    "        \"thus\",\n",
    "        \"to\",\n",
    "        \"together\",\n",
    "        \"too\",\n",
    "        \"top\",\n",
    "        \"toward\",\n",
    "        \"towards\",\n",
    "        \"twelve\",\n",
    "        \"twenty\",\n",
    "        \"two\",\n",
    "        \"un\",\n",
    "        \"under\",\n",
    "        \"until\",\n",
    "        \"up\",\n",
    "        \"upon\",\n",
    "        \"us\",\n",
    "        \"very\",\n",
    "        \"via\",\n",
    "        \"was\",\n",
    "        \"we\",\n",
    "        \"well\",\n",
    "        \"were\",\n",
    "        \"what\",\n",
    "        \"whatever\",\n",
    "        \"when\",\n",
    "        \"whence\",\n",
    "        \"whenever\",\n",
    "        \"where\",\n",
    "        \"whereafter\",\n",
    "        \"whereas\",\n",
    "        \"whereby\",\n",
    "        \"wherein\",\n",
    "        \"whereupon\",\n",
    "        \"wherever\",\n",
    "        \"whether\",\n",
    "        \"which\",\n",
    "        \"while\",\n",
    "        \"whither\",\n",
    "        \"who\",\n",
    "        \"whoever\",\n",
    "        \"whole\",\n",
    "        \"whom\",\n",
    "        \"whose\",\n",
    "        \"why\",\n",
    "        \"will\",\n",
    "        \"with\",\n",
    "        \"within\",\n",
    "        \"without\",\n",
    "        \"would\",\n",
    "        \"yet\",\n",
    "        \"you\",\n",
    "        \"your\",\n",
    "        \"yours\",\n",
    "        \"yourself\",\n",
    "        \"yourselves\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f84523-db18-4a2f-87c3-559324dec82a",
   "metadata": {},
   "source": [
    "### Identifying frequent words to add to custom stopword list\n",
    "\n",
    "We could also generate a document-term-matrix and inspect the most frequent words in the vocabulary that occur across all texts. This can help us add further to our stopwords list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6060d3b0-0a08-4d28-a94b-86d4898fc7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 600)\n",
    "from pathlib import Path  \n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a67ba4d-5a24-41e6-8889-cd0d1c8e8a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "directory_path = 'kafka-corpus/'\n",
    "text_files = glob.glob(f'{directory_path}/*.txt')\n",
    "text_titles = [Path(text).stem for text in text_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f118d85d-ae6c-4941-b447-7a9d39c7a177",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up CountVectorizer()\n",
    "cv=CountVectorizer(input='filename', stop_words=custom_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a46bf37-d4dd-4e7d-ae67-1f85d753bec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate document-term matrix for the docs\n",
    "#DTM reminder: \n",
    "#each row is a document\n",
    "#each column is an element (word) of the total vocabulary for the corpus\n",
    "dtm=cv.fit_transform(text_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c55aa0-80e0-4242-81f1-a1615a739116",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sorting vocabulary in order of counts\n",
    "#first we create a dictionary with words in the vocabulary as keys and counts as values\n",
    "dictVocab = {}\n",
    "for x in range(dtm.shape[1]):\n",
    "    dictVocab[cv.get_feature_names_out()[x]]=dtm.toarray().sum(axis=0)[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e11d3d8-e106-40b9-979e-56e7a43ab10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#then we sort the dictionary in order of counts\n",
    "sortVocab = sorted(dictVocab.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d29e10-00e1-4c2b-85fc-706f11323c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#then we print top 30 words with highest frequency counts across all texts\n",
    "for i in sortVocab[0:30]:\n",
    "    print(i[0], i[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df862611-face-4932-99dd-c5bfdb1adaf4",
   "metadata": {},
   "source": [
    "<a id='section-4'></a>\n",
    "## A note on tokenizing and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7036c40e-2bc2-418d-a227-7a9ddae9563d",
   "metadata": {},
   "source": [
    "When using Scikit-Learn for calculating tf-idf, certain pre-processing steps are set as defaults.  \n",
    "\n",
    "As we’ve discussed, defaults in text analysis tools are often suited to assumptions about how the English language works, and this might not be the best way to do things depending what language you’re working with and what your research goals are. \n",
    "\n",
    "We’ll have a look here at Scikit-learn’s preprocessing defaults, and how to modify them. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebd9ca8-797a-4301-93ae-f180957761bb",
   "metadata": {},
   "source": [
    "#### Lowercasing\n",
    "\n",
    "The default parameters for lowercasing is `lowercase=True` which means that text will be converted to lower case. Specify parameter `lowercase=False` if you don’t want to lower case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9830df83-521a-4d8f-9350-876994ffe12a",
   "metadata": {},
   "source": [
    "#### Tokenizing\n",
    "\n",
    "There are two ways you can modify the built-in tokenizing process. \n",
    "\n",
    "You can override the default tokenizing by defining your own tokenizing function and calling it as parameter: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bbb0d0-0fd3-4ca6-b71f-61afe10fbf4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call tokenizing function\n",
    "import re\n",
    "\n",
    "def tokenize(text):\n",
    "    lowercase_text = text.lower()\n",
    "    split_words = re.split('\\W+', lowercase_text)\n",
    "    tokenized = [word for word in split_words if word.isalpha()]\n",
    "    return tokenized\n",
    "\n",
    "#Set up vectorizer\n",
    "tfidf = TfidfVectorizer(input='filename',\n",
    "                        tokenizer=tokenize,\n",
    "                        stop_words=custom_stopwords,\n",
    "                        lowercase=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf69725-bedd-49f2-ae54-b89a6ed51c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Actually do the vectorizing\n",
    "tfidf_vector = tfidf.fit_transform(text_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b547ba6-56bc-4e1e-90b3-8ddf9fab21a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a DataFrame out of the resulting tf–idf vectors, \n",
    "#setting the “feature names” (words in vocabulary) as columns and the titles as rows\n",
    "tfidf_df = pd.DataFrame(tfidf_vector.toarray(), \n",
    "                        index=text_titles, \n",
    "                        columns=tfidf.get_feature_names_out())\n",
    "tfidf_df.head(11)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f56245a-327d-451e-9347-f3d12778dd27",
   "metadata": {},
   "source": [
    "Or you could change the built-in tokenizing pattern directly.  \n",
    "\n",
    "The default tokenizing patterns is `token_pattern=r\"(?u)\\b\\w\\w+\\b\"`. This pattern selects tokens of 2 or more alphanumeric characters (punctuation is completely ignored and always treated as a token separator).\n",
    "\n",
    "You could replace this pattern with your own regular expression. For example, `token_pattern=r\"(?u)\\W+\"`. This pattern would split characters at anything that is not a word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bc0beb-fa3a-42ae-9a58-70e923060a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace tokenizing pattern\n",
    "tfidf = TfidfVectorizer(token_pattern=r'(?u)\\W+')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d838ce-283c-4845-8d25-96161a02b24d",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "\n",
    "Since tf-idf relies on word counts in order to identify words that appear in particular documents compared to other documents, for languages that are highly inflected it might be helpful to lemmatize the text in order to improve the words counts. There are no built-in lemmatizers. We need to use other libraries to lemmatize the text before calculating tf-idf scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa3ce2c-3012-4728-93cd-076172df8bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatizing using spaCy for German\n",
    "import spacy\n",
    "!python -m spacy download de_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6424a4-bf07-40c4-a708-4f4241087826",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load language model\n",
    "nlp = spacy.load('de_core_news_sm')\n",
    "\n",
    "#Open your text and create spaCy document\n",
    "filepath = 'kafka_dv.txt'\n",
    "text = open(filepath, encoding='utf-8').read()\n",
    "document = nlp(text)\n",
    "\n",
    "outname = filepath.replace('.txt', '-lemmatized.txt')\n",
    "with open(outname, 'w', encoding='utf8') as out:   \n",
    "    for token in document:\n",
    "        # Get the lemma for each token\n",
    "        out.write(token.lemma_.lower())\n",
    "        # Insert white space between each token\n",
    "        out.write(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64982e55-776a-4de7-b3be-91a45fee4e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatizing using spaCy for French\n",
    "import spacy\n",
    "!python -m spacy download fr_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f746b593-37f3-4c00-864c-097a0767e00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load language model\n",
    "nlp = spacy.load('fr_core_news_sm')\n",
    "#And follow process above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bfef8e-b91c-414b-ba12-18515c1ae2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatizing using spaCy for Spanish\n",
    "import spacy\n",
    "!python -m spacy download es_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc1afc4-cd42-4311-8979-95d35ecabf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load language model\n",
    "nlp = spacy.load('es_core_news_sm')\n",
    "#And follow process above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49b697d-d9a6-42bb-850e-e137b05a8a12",
   "metadata": {},
   "source": [
    "<a id='section-5'></a>\n",
    "## Playing with Tf-idf parameters\n",
    "\n",
    "Now let's go through the process of identifying significant words with tf-idf, this time looking at parameters more closely, and using a custom stopwords list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7745821c-d3b3-433b-b779-a55409f3cd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Parameters\n",
    "The parameters you choose affect the output. \n",
    "these settings all have pros and cons; \n",
    "there’s no singular, correct way to preset them and produce output. \n",
    "Instead, it’s best to understand what settings do so that you can describe \n",
    "and defend the choices you’ve made. \n",
    "The full list of parameters is described in Scikit-Learn’s documentation.\n",
    "\n",
    "input='filename' so we can pass a list of files that the vectorizer will read and\n",
    "fetch the raw content to analyze. default: input='content'\n",
    "\n",
    "min_df, max_df\n",
    "These settings control the minimum number of documents a term must be found in to be included \n",
    "and the maximum number of documents a term can be found in in order to be included. \n",
    "Either can be expressed as a decimal between 0 and 1 indicating the percent threshold, \n",
    "or as a whole number that represents a raw count. \n",
    "Setting max_df below .9 will typically remove most or all stopwords.\n",
    "e.g. max_df=0.75 will limit to terms appearing in 75 percent of the documents or lower.\n",
    "\n",
    "norm, smooth_idf, and sublinear_tf\n",
    "Each of these will affect the range of numerical scores that the tf-idf algorithm outputs.\n",
    "norm normalizes the scores, default: norm='l2'\n",
    "Smooth-idf adds one to each document frequency score, default: smooth_idf=True\n",
    "Sublinear_tf applies another scaling transformation, replacing tf with log(tf). \n",
    "default: sublinear_tf=False\n",
    "It is recommended to keep the default norm and smooth_idf paramters, this will better account \n",
    "for differences in text length and overall produce more meaningful tf–idf scores.\n",
    "\n",
    "strip_accent='unicode' will remove accents and perform other character normalization \n",
    "during the preprocessing step. default: strip_accent='None'\n",
    "\n",
    "We've already mentioned above the stop_words, lowercase, tokenizer, and token_pattern parameters.\n",
    "\"\"\"\n",
    "#Set up tf-idf vectorizing with custom settings\n",
    "tfidf_vectorizer = TfidfVectorizer(input='filename')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f7d671-e57c-4d79-b349-ad6203c6207a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Actually do the vectorizing\n",
    "tfidf_vector = tfidf_vectorizer.fit_transform(text_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c633734-2db4-4de2-8eeb-73e767161117",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a DataFrame out of the resulting tf–idf vector, \n",
    "#setting the “feature names” or words as columns and the titles as rows\n",
    "\"\"\"\n",
    "N.B. The fit_transform() method above converts the list of strings to something called a sparse matrix. \n",
    "Sparse matrices save on memory by leaving out all zero values, but we want access to those, \n",
    "so the next block uses the toarray() method to convert the sparse matrices to a numpy array. \n",
    "By converting the space matrix to an array, we ensure that our array is the same length as our\n",
    "list of documents. \n",
    "We want every term represented so that each document has the same number of values, \n",
    "one for each word in the corpus even if it doesn't occur in some documents.\n",
    "\"\"\"\n",
    "\n",
    "tfidf_df = pd.DataFrame(tfidf_vector.toarray(), \n",
    "                        index=text_titles, \n",
    "                        columns=tfidf_vectorizer.get_feature_names_out())\n",
    "tfidf_df.head(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4490041-9941-4791-9900-2a028fb764ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add row for document Tf-idf (sum of tf-idf score for each word across all documents)\n",
    "tfidf_df.loc['00_Document Tf-idf'] = tfidf_df.sum(axis=0)\n",
    "tfidf_df.head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f29889-8307-4d5a-8da0-3adbf9da7ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort index (and you could also round to only 2 decimals\n",
    "#but since our score are very low we'll leave them as is\n",
    "tfidf_df = tfidf_df.sort_index()#.round(decimals=2)\n",
    "tfidf_df.head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f7a406-2d02-4468-8fe2-20bd49e15c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Re-organize so words are in rows rather than columns\n",
    "stacked_tfidf_df = tfidf_df.stack().reset_index()\n",
    "stacked_tfidf_df = stacked_tfidf_df.rename(columns={0:'tfidf', 'level_0': 'document','level_1': 'term', 'level_2': 'term'})\n",
    "stacked_tfidf_df.sample(n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371dedc1-c8ac-4693-a701-a259940e641d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Top 10 words with the highest tf–idf for every story\n",
    "top_tfidf = stacked_tfidf_df.sort_values(by=['document','tfidf'], ascending=[True,False]).groupby(['document']).head(10)\n",
    "top_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450b3902-d154-411d-9888-2f6400e86e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Zoom in on particular words\n",
    "#What documents have the given word in their top significant words?\n",
    "top_tfidf[top_tfidf['term'].str.contains('people')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a074f7-e8e8-4513-8a3b-98a0cdf08c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display signficance scores of the given word across all documents\n",
    "stacked_tfidf_df[stacked_tfidf_df['term'].str.contains('people')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba004178-ad00-440c-81e8-707ff5afa68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Zoom in on particular document\n",
    "#What are the top significant words for a given document?\n",
    "top_tfidf[top_tfidf['document'].str.contains('kafka_a-hunger-artist')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb95a6e-cea8-46e7-a88d-10e7d53a3c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#What are the top 20 significant words for the given document?\n",
    "(stacked_tfidf_df[stacked_tfidf_df['document']\n",
    "                  .str.contains('kafka_a-hunger-artist')]\n",
    " .sort_values('tfidf', ascending=False)\n",
    " .head(20)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4f4d34-700f-445f-8e56-a666ee817ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create bar plots of top 10 significant words for all documents, and each story\n",
    "import seaborn as sns\n",
    "sns.catplot(data=top_tfidf, row='document', x='tfidf', y='term', kind='bar', sharey=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4b6d83-9ff1-4e39-8e57-6d147ea3d66f",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='section-6'></a>\n",
    "## Interpreting Tf-idf outputs\n",
    "\n",
    "Tf-idf outputs can suggest hypotheses and spark questions that you want to look into further. They constitute a first step for identifying patterns that can be investigated further.\n",
    "For example:  \n",
    "\n",
    "- What does it mean that a certain term is significant to a certain document? What are the actual contexts the term appears in?  \n",
    "- What role does the term play in the narrative?  \n",
    "- How does it compare to other stories: do other stories include instances of the term? Do they include similar concepts but not that term? If so what are those concepts?  \n",
    "\n",
    "Interpretations can be developed further by combining with other methods (ngrams, vector comparison) and other modes of reading (contextualized reading, close reading). Tf-idf in particular is both a corpus exploration method in itself and a pre-processing step for many other text-mining measures and models. As we heard from [Ari's presentation](https://drive.google.com/file/d/1lLLFzeCMUDKClrm7UqyIgpdX3b_P9zjO/view?usp=sharing), tf-idf scores can be used instead of word counts to generate word vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f0e9a8-019c-4e3f-83c6-112b1b2bd5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use tf-idf scores and cosine distance to compare documents\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "# Data: set up path to files and a variable with file names\n",
    "directory_path = 'kafka-corpus/'\n",
    "text_files = glob.glob(f'{directory_path}/*.txt')\n",
    "text_titles = [Path(text).stem for text in text_files]\n",
    "\n",
    "#Set up tf-idf vectorizing with custom settings\n",
    "tfidf_vectorizer = TfidfVectorizer(input='filename')\n",
    "\n",
    "#Actually do the vectorizing\n",
    "tfidf_vector = tfidf_vectorizer.fit_transform(text_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4764e3c9-2fa7-41ca-8a50-d05a50e84fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assessing similarties between texts based on tf-idf scores\n",
    "#Creates a dataframe with cosine distances between the texts\n",
    "#calculated from vectors of word counts for each text\n",
    "#cosine distance (as opposed to cosine similar): the closer to 0, the more similar\n",
    "cosine_distances = pd.DataFrame(squareform(pdist(tfidf_vector.toarray(), metric='cosine')), \n",
    "                                columns=text_titles, index=text_titles)\n",
    "cosine_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d237d4-e896-4e79-8e57-f552c86e0a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize the cosine distance between texts with heatmap based on significance scores\n",
    "import seaborn as sns\n",
    "sns.heatmap(data=cosine_distances, annot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71f2354-9f16-4163-a100-c3152884bc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize relations between texts with cluster map\n",
    "sns.clustermap(data=cosine_distances, annot=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a060a6f2-f08e-4d80-b83a-636e4a58cd3e",
   "metadata": {},
   "source": [
    "_Acknowledgements_: This notebook is inspired by Melanie Walsh’s [_Introduction to Cultural Analytics & Python_](https://melaniewalsh.github.io/Intro-Cultural-Analytics/05-Text-Analysis/01-TF-IDF.html) and Matthew Lavin's [\"Analyzing Documents with TF-IDF](https://programminghistorian.org/en/lessons/analyzing-documents-with-tfidf)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
