{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2eac105a-242b-40fb-8231-3d365ec3825b",
   "metadata": {},
   "source": [
    "# Topic Modeling in Gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9c1fe0-4ade-41e5-a98f-101d07873966",
   "metadata": {},
   "source": [
    "## Introduction to Topic Modeling\n",
    "\n",
    "Topic modeling is a more obscure process than, for example, TF-IDF.  TF-IDF is rules-based and the algorithm is transparent, whereas topic modeling is an unsupervised approach.\n",
    "\n",
    "Topic modeling sorts words together based on co-occurrence probabilities. It sorts words from a collection of document into likely clusters of co-occurring words, i.e. how often words likely appear close to one another. These are the topics.\n",
    "\n",
    "You determine that there is a certain number of topics that constitutes each document. The model then estimates the probability of each word in the documents of belonging to a topic. It iterates over and over adjusting each time its probability scores. \n",
    "\n",
    "As with other text analysis methods, there are different algorithms for implementing the topic modeling process. One of the most popular implementations of topic modeling is LDA (Latent Dirichlet Allocation)\n",
    "\n",
    "LDA’s approach to topic modeling is that it considers each document as a collection of topics in a certain proportion. And each topic as a collection of keywords, again, in a certain proportion. Once you provide the algorithm with the number of topics, all it does it to rearrange the topics distribution within the documents and keywords distribution within the topics to obtain a good composition of topic-keywords distribution. LDA is non-deterministic. This means that a different result can be obtained for each run. Fortunately, the result usually converges towards a stable state. Run it several times and compare the results.\n",
    "\n",
    "This notebook covers LDA implementation in Gensim. [MALLET](https://mimno.github.io/Mallet/) is another popular LDA implementation - some argue it is more efficient and give better topic segregation. It’s also quite intuitive to use, but it’s difficult to set up through JupyterHub.\n",
    "\n",
    "\n",
    "**Topic Modeling Outputs** \n",
    "\n",
    "In this notebook we’ll look at two main topic modeling outputs:\n",
    "\n",
    "_Topics (probability distributions of words over documents)_: each topic is a list of likely frequently co-occurring terms and each word more or less characteristic of the topic\n",
    "\n",
    "_Distributions of topics over documents_: list the more characteristic topics for each topic and how strongly the document is associated with the topic\n",
    "\n",
    "> “A topic model provides a different perspective on a collection. It creates a set of probability distributions over the vocabulary of the collection, which, when combined together in different proportions, best match the content of the collection. We can sort the words in each of these distributions in descending order by probability, take some arbitrary number of most-probable words, and get a sense of what (if anything) the topic is “about.” Each of the text segments also has its own distribution over the topics, and we can sort these segments by their probability within a given topic to get a sense of how that topic is used.” (Nguyen et al. “How We Do Things With Words”, p. 9)\n",
    "\n",
    "\n",
    "**Interpreting topic models**\n",
    "\n",
    "Topic modeling is useful to get a sense of the patterns across a corpus and to identify patterns you might want to look into further. \n",
    "\n",
    "> “Topic models (e.g., LDA, Blei et al., 2003) are usually unsupervised and therefore less biased toward human-defined categories. They are especially suited for insight-driven analysis, because they are constrained in ways that make their output interpretable. Although there is no guarantee that a “topic” will correspond to a recognizable theme or event or discourse, they often do so in ways that other methods do not. Their easy applicability without supervision and ready interpretability make topic models good for exploration. Topic models are less successful for many performance-driven applications. Raw word features are almost always better than topics for search and document classification. LSTMs and other neural network models are better as language models. Continuous word embeddings have more expressive power to represent fine-grained semantic similarities between words.” (Nguyen et al. “How We Do Things With Words”, p. 9)\n",
    "\n",
    "Topic modeling was introduced to the wider public as an information retrieval and categorization tool: as a way of categorizing documents by identifying its overarching themes. This created a sense that topic modeling outputs coherent categories that represent themes in documents. \n",
    "\n",
    "But it’s important to keep in mind that topic models identify consistent patterns across a corpus. They will identify features of language that likely frequently co-occur. These patterns can seem like “topics”, like coherent themes, because (according to the distributional hypothesis) words that share the same context, words that frequently occur close together, \n",
    "generate meaning, are proxies of meaning. When a document contains words such as “telescope”, “sky”, “planet”, “star” this can indicate that the document probably relates to astronomy. However, a document that contains words such as “star”, “famous”, “fans”, “film” suggests a different kind of meaning, or topic being discussed. Topic modeling picks up on these different groupings. \n",
    "\n",
    "> “Indeed calling these models “topic models” is retrospective — the topics that emerge from the inference algorithm are interpretable for almost any collection that is analyzed. The fact that these look like topics has to do with the statistical structure of observed language and how it interacts with the specific probabilistic assumptions of LDA.” (Blei, “Probabilistic Topic Models” footnote p. 79)\n",
    "\n",
    "But consistent patterns of words don’t necessarily simply equate with themes or topics, what a text is “about”. If there is one document or set of documents in your corpus that consistently repeats particular words or phrases this will be picked up as a pattern. Sometimes it’s significant, sometimes it’s not. For example:\n",
    "\n",
    "- If you are comparing documents that are of different types (e.g. newspapers v transcripts or plays v novels etc.) models will likely pick up on different language conventions that typify each genre which may or may not be relevant for your analysis. Similarly, if some document contain distinctive language or dialect features these can be grouped together as a topic. \n",
    "\n",
    "> “Longer or extended poems that outsize the majority of other documents in the subset pull one or more topics toward language specific to that particular poem. (…) one poem with high levels of repetition can pull a topic away from the rest of the corpus, along with other poems with high frequency repetitions of particular phrases” (Rhody, “Topic Modeling and Figurative Language”)\n",
    "\n",
    "- Models can pick up on recurring preprocessing errors, e.g. if there are OCR misspelling or words repeatedly split at the end of lines \n",
    "\n",
    "> “After fitting the model, it may be necessary to circle back to an earlier phase. Topic models find consistent patterns. When authors repeatedly use a particular theme or discourse, that repetition creates a consistent pattern. But other factors can also create similar patterns, which look as good to the algorithm. We might notice a topic that has highest probability on French stopwords, indicating that we need to do a better job of filtering by language. We might notice a topic of word fragments, such as “ing,” “tion,” “inter,” indicating that we are not handling end-of- line hyphenation correctly. We may need to add to our stoplist or change how we curate multi-word terms.” (Nguyen et al. “How We Do Things With Words”, p. 10)\n",
    "\n",
    "**Tuning Topic Models**\n",
    "\n",
    "One of the main challenges with topic modeling is generating outputs that are meaningful and relevant. It will take some experimentation. Two of the most important aspects for generating meaningful topics is preprocessing and the number of topics. There is no general rule for how many topic is a good number of topics. This will take some tuning and experimentation.\n",
    "\n",
    "> “One of the most common questions about topic models is how many topics to use, usually with the implicit assumption that there is a “right” number that is inherent in the collection. We prefer to think of this parameter as more like the scale of a map or the magnification of a microscope. The “right” number is determined by the needs of the user, not by the collection. If the analyst is looking for a broad overview, a relatively small number of topics may be best. If the analyst is looking for fine-grained phenomena, a larger number is better.” (Nguyen et al. “How We Do Things With Words”, p. 9-10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891c76a6-1bba-47d5-87e6-0b510dbd0401",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc10334-64ea-46f2-9fc5-bf4c44e246b0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "!python -m spacy download en_core_web_md\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "from pathlib import Path  \n",
    "import glob\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "\n",
    "#import pyLDAvis\n",
    "#import pyLDAvis.gensim_models as gensimvis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f94ebc-e71b-4d85-b4e1-88bc5e4bb23d",
   "metadata": {},
   "source": [
    "## 1-Setting up for Building the Models\n",
    "\n",
    "There's some set-up involved for using Genism to create topic models. \n",
    "\n",
    "If we follow some preprocessing steps (most of which should be familiar to us by now) this will increase the quality of the models. \n",
    "\n",
    "Gensim also expects the data to be structured in a certain way to generate models (as a dictionary and corpus created from that dictionary).\n",
    "\n",
    "We'll go over these steps which involve: preprocessing then making a dictionary and corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ad10bf-b918-4c75-bcc3-2d7618b3a6af",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cd8efa-b96d-43c3-95db-bb38d6982e05",
   "metadata": {},
   "source": [
    "**Lemmatize the files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ed9004-8a84-43d1-a0e1-cb4004c6d14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This loops over multiple files in a directory\n",
    "#but it might make the kernel crash if it runs out memory\n",
    "#If the kernel crash you might have to lemmatize single files at a time (cf. below)\n",
    "\n",
    "#Load language model (it needs to match the name above)\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "#Open your texts and create spaCy document\n",
    "filepath = 'kafka-corpus/'\n",
    "text_files = glob.glob(f'{filepath}/*.txt')\n",
    "\n",
    "#Loop through the files and open as spacy document\n",
    "for file in text_files:\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "        print(file)\n",
    "        document = nlp(text)\n",
    "        \n",
    "    #Lemmatize\n",
    "    outname = file.replace('.txt', '-lemmatized.txt')\n",
    "    with open(outname, 'w', encoding='utf8') as out:   \n",
    "        for token in document:\n",
    "            # Get the lemma for each token\n",
    "            out.write(token.lemma_.lower())\n",
    "            # Insert white space between each token\n",
    "            out.write(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf10be72-9590-491c-bc01-650eba7da55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatize single files\n",
    "#Kernel crashes for large single files\n",
    "#Either chunk the text into smaller text units and lemmatize\n",
    "#Or skip lemmatization phase\n",
    "\n",
    "#Load language model (it needs to match the name above)\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "#Open your text and create spaCy document\n",
    "filepath = 'kafka-corpus/kafka_the-trial.txt'\n",
    "text = open(filepath, encoding='utf-8').read()\n",
    "document = nlp(text)\n",
    "\n",
    "outname = filepath.replace('.txt', '-lemmatized.txt')\n",
    "with open(outname, 'w', encoding='utf8') as out:   \n",
    "    for token in document:\n",
    "        # Get the lemma for each token\n",
    "        out.write(token.lemma_.lower())\n",
    "        # Insert white space between each token\n",
    "        out.write(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64448942-dc20-41b7-bc95-6878f6571008",
   "metadata": {},
   "source": [
    "**Tokenize your text either using gensim built-in tokenizing or using your own tokenizing function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20223f98-324c-4ed8-9fd1-7928e4510279",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tokenize using gensim built-in tokenization\n",
    "\n",
    "#Put all texts into a single list\n",
    "#Loop through the texts and tokenize them with custom tokenizing function\n",
    "directory_path = 'kafka-corpus/'\n",
    "all_docs = []\n",
    "\n",
    "for filepath in Path(directory_path).glob(\"*.txt\"):\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "        tokenized_text = gensim.utils.simple_preprocess(text)\n",
    "        all_docs.append(tokenized_text)\n",
    "\n",
    "#See the first document as tokenized list of words\n",
    "all_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3aba8a7-c882-4805-9f56-68dc7137d25f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tokenize using cutsom tokenizing function\n",
    "\n",
    "#Put all texts into a single list\n",
    "#Loop through the texts and tokenize them with custom tokenizing function\n",
    "from pathlib import Path\n",
    "directory_path = 'kafka-corpus/'\n",
    "all_docs = []\n",
    "\n",
    "def tokenize(text):\n",
    "    lowercase_text = text.lower()\n",
    "    split_words = re.split(r'\\W+', lowercase_text)\n",
    "    tokenized = [word for word in split_words if word.isalpha()]\n",
    "    return tokenized\n",
    "\n",
    "for filepath in Path(directory_path).glob(\"*.txt\"):\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "        tokenized_text = tokenize(text)\n",
    "        all_docs.append(tokenized_text)\n",
    "\n",
    "#See the first document as tokenized list of words\n",
    "all_docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f6826b-f72e-4fa8-869e-d196a0caa967",
   "metadata": {},
   "source": [
    "**Remove stopwords**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34b1e75-0621-4fe3-863d-0c18b502e790",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Stopwords: refer to \"Preprocessing\" notebook for more details on stopwords\n",
    "\n",
    "#Load custom stopwords list (this is the default spacy list)\n",
    "#open your txt file and convert to a Python list\n",
    "with open(\"custom-stopwords.txt\", \"r\") as file_object:\n",
    "    custom_stopwords = [s.rstrip('\\n') for s in file_object.readlines()] \n",
    "\n",
    "custom_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06243f41-b990-4d98-8e76-f8b0c6744ea5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(list_of_tokens, stopwords):\n",
    "    return [token for token in list_of_tokens if token not in stopwords]\n",
    "\n",
    "all_docs_no_stop = []\n",
    "\n",
    "for file in all_docs: \n",
    "    nostop = remove_stopwords(file, custom_stopwords)\n",
    "    all_docs_no_stop.append(nostop)\n",
    "    \n",
    "all_docs_no_stop[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6977ddc3-48e5-41f8-8817-1f123673b8a6",
   "metadata": {},
   "source": [
    "## Creating Bigrams and Trigrams\n",
    "\n",
    "Bigrams are two words frequently occurring together that need to be grouped together to make sense (e.g. \"black hole\", \"European Union\"). Trigrams are 3 words frequently occurring together that need to be grouped together to make sense. Identifying bigrams and trigrams in our corpus will improve the quality of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f6dbf8-ead4-4428-b11f-db8a718eae96",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models\n",
    "# min_count: minimum number of times words occur together to be considered a bigram\n",
    "# threshhold: the higher the number the fewer number of ngrams will be identified\n",
    "bigram = gensim.models.Phrases(all_docs_no_stop, min_count=5, threshold=100)\n",
    "trigram = gensim.models.Phrases(bigram[all_docs_no_stop], threshold=100)\n",
    "\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "data_bigrams = make_bigrams(all_docs_no_stop)\n",
    "data_bigrams_trigrams = make_trigrams(data_bigrams)\n",
    "\n",
    "#You can find the ngram by searching for words linked with underscore \n",
    "#(command + F and search for underscore)\n",
    "#If you're not staisfied with the bigrams you're getting (capturing too many\n",
    "#or too few then modify the min_count and threshhold parameters\n",
    "print(data_bigrams_trigrams[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d890f3e1-2656-4a24-9f04-cbfe14ba35cc",
   "metadata": {},
   "source": [
    "## Create Corpus and Dictionary needed for Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786a12ca-4247-4159-be50-641930340c56",
   "metadata": {},
   "source": [
    "Creating a dictionary from the corpus restructures the text in a way that prepares it for topic modeling. In this stage we create an id (key) for each unique word in a document and associate it with the frequency of the word in a document. The dictionary create the unique ids (keys) and the corpus maps the word ids to their frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2bf0a5-6319-4370-a107-b4d966dedcd4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create Dictionary (associates a key to each unique word)\n",
    "id2word = corpora.Dictionary(data_bigrams_trigrams)\n",
    "\n",
    "# Create Corpus (associates word frequency with each key for each unique word from Dictionary\n",
    "corpus = []\n",
    "for text in data_bigrams_trigrams:\n",
    "    new = id2word.doc2bow(text)\n",
    "    corpus.append(new)\n",
    "\n",
    "#Print corpus for first document\n",
    "#You will see a list of (unique word ID, and its frequency)\n",
    "print (corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096aa712-1647-4c34-b722-d953d7384aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#See what word is associated with a particular key/ID number\n",
    "print(id2word[39])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2faf1514-340e-4fd1-af96-65c1a25b2352",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Human readable format of corpus (term and its frequency)\n",
    "#List the unique words, and their frequency for first document\n",
    "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135765bb-3c7a-4691-b1fe-4f5de7f554cb",
   "metadata": {},
   "source": [
    "# 2-Building the Topic Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b88c19-3ba4-4bb8-b3d8-3ce56ce8cf1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Parameters: \n",
    "corpus and dictionary we created above\n",
    "\n",
    "num_topics is the number of topics\n",
    "\n",
    "passes: total number of training passes, the number of passes through training data\n",
    "\n",
    "chunksize: the number of documents to be used in each training chunk\n",
    "\n",
    "update_every: determines how often the model parameters should be updated\n",
    "\n",
    "See gensim documentation for more\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=5, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=100,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e735a246-1492-496f-8554-9fcebd54e467",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Topics (probability distributions of words across the corpus)\n",
    "#This list the topics (Topic 0,1,2 etc.)\n",
    "#and print the list of words most characteristic for each topic\n",
    "#preceded by its proability score (how strongly it is characteristic of the topic)\n",
    "#change the num_words to get more or less words for each topic\n",
    "pprint(lda_model.print_topics(num_words = 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fc5be0-8127-4e6d-bfd8-832bfaa98494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distributions of topics over documents: \n",
    "#what topics are associated with each document\n",
    "#This returns a list of each document which lists the most characteristic topics \n",
    "#for that document and their weight of association (topic proability) with that document\n",
    "\n",
    "topics_per_document=[lda_model.get_document_topics(item) for item in corpus]\n",
    "topics_per_document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610bb752-ce09-4a29-b2b6-d6ae7c5c930d",
   "metadata": {},
   "source": [
    "## Visualizing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332f6d75-525e-484a-b732-6fb3c85a543b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "lda_display = gensimvis.prepare(lda_model, corpus, id2word)\n",
    "pyLDAvis.display(lda_display)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbded35-aefa-4a91-90a8-c802c09c7ea0",
   "metadata": {},
   "source": [
    "Each bubble on the left-hand side of the plot represents a topic. The larger the bubble, the more prevalent is that topic and the more documents associated with that topic.\n",
    "\n",
    "A good topic model will have fairly big, non-overlapping bubbles scattered throughout the chart instead of being clustered in one quadrant. Although slightly overlapping topics is not a bad thing: they reveal connections between topics.\n",
    "\n",
    "A model with too many topics, will typically have many overlaps, small sized bubbles clustered in one region of the chart.\n",
    "\n",
    "If you move the cursor over the bubbles, the words and bars on the right-hand side will update. These words are the words characteristic of that topic.\n",
    "\n",
    "This visualization can give you sense of how you can tune your models by adjusting your parameters (e.g. increasing or decreasing number of topics). If words are not meaningful can also add them to custom stopwords list (cf. Preprocessing notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84421ea1-b8f0-4fed-8461-0777dd5c53e6",
   "metadata": {},
   "source": [
    "## Tuning topic models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9411f80-cdd7-45da-94e5-f6ea160e95bf",
   "metadata": {},
   "source": [
    "You can refine your models using the visualization above. If you want to get more in-depth you could use perplexity and coherence scores. These measure how coherent topics are (the more coherence, the more meaningful and interpretable the topics). They can be heplful gives to identifying an optimal number of topics. You can try building different models with different number of topics and seeing how the scores change (especially the coherence score)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2325859-33b9-4326-828c-444fabd6262f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Perplexity\n",
    "# the lower the better.\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711dd140-dc10-46cd-9fbe-8b95ff2d1d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Coherence Score\n",
    "#the higher the better\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_bigrams_trigrams, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4930915f-54ac-4ed5-bff4-522f482dc236",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
