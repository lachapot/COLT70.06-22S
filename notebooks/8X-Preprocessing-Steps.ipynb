{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4eb2b451-71ea-4412-9358-5e87ba0fdbe1",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f43014f-9850-4efd-a138-81a024f3f22f",
   "metadata": {},
   "source": [
    "**Contents**\n",
    "\n",
    "[Step 1: Normalizing and building a stopwords list](#section-1)\n",
    "\n",
    "[Step 2: Lemmatization](#section-2)\n",
    "\n",
    "[Step 3: Textual units of analysis: Chunking](#section-3)\n",
    "\n",
    "[Step 4: Tokenizing](#section-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb620d5-95b8-43e1-bd03-d5ef2f2e8047",
   "metadata": {},
   "source": [
    "<a id='section-1'></a>\n",
    "## Step 1: Normalizing and building a stopwords list\n",
    "\n",
    "### Are there any strange characters or strange things in the data that I need to replace or get rid of?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc6972a-b91b-47d6-adf6-3662e7375ed3",
   "metadata": {},
   "source": [
    "Do a character count in order to see if there are any strange characters I might want to delete or replace: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a27e219-c503-4169-91ed-c5ff444653fa",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Counting characters in a text\n",
    "from collections import Counter\n",
    "\n",
    "f = open('kafka_dv.txt', 'r')\n",
    "test_text = f.read()\n",
    "Counter(test_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a236b519-59cb-43a0-90e8-17f2854512b9",
   "metadata": {},
   "source": [
    "I see there are a lot of newline characters (`\\n`). I'm going to replace those with a space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2045a2da-e6b8-49ba-9384-8327cead2091",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_text.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961ceea9-984c-435d-86c3-6151c63c1a7c",
   "metadata": {},
   "source": [
    "**Normalizing encodings**\n",
    "\n",
    "\n",
    "Character counts can also flag up any encoding issues. Especially when working with language that have accented characters, we can see here if there are any problems with those accented characters (for example, if the \"same\" accented characters don't in fact share the same encoding). Then we can normalize the data to ensure all characters are encoded properly so that it doesn't mess up our counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0dfbef6-0bff-4022-af27-93c7c385e07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For example, here we have the letter \"a\" with acute accent\n",
    "# Its unicode value in python is 225\n",
    "#and it has a length of 1: it is one accented character\n",
    "import unicodedata\n",
    "char = \"치\"\n",
    "print(ord(char))\n",
    "len(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078e3a1c-c82b-4ffa-b089-b2a2385e5a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The unicode name for this character\n",
    "[ unicodedata.name(c) for c in char ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2598ba04-9c34-4803-a1bd-dc5490a8fde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#But a character that looks the same could be encoded differently\n",
    "#Here we have the letter a with acute accent again\n",
    "#but it is in fact the combination of two unicode code-points (97 + 769) \n",
    "#so it has a length of 2\n",
    "char2 = \"a패\"\n",
    "print([ord(c) for c in char2])\n",
    "len(char2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3231275c-ef74-4d1e-8f82-525c322dc0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The unicode name for this character\n",
    "[ unicodedata.name(c) for c in char2 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bca9963-aacd-4af1-8659-cc4584060a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#These characters look the same but are not infact the same, and have different lengths\n",
    "len(char) == len(char2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a56550d-e815-42ce-8add-bc8d8ff16f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Different character encodings will be counted as different characters\n",
    "'h치lo friend' == 'ha패lo friend'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96adc7c-c428-49b0-931f-30e9e285a4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can use the .normalize() method to normalize the character encodings\n",
    "test = char + char2\n",
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec58d12c-fc0f-4fe4-ab25-16bbfc2eb848",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize all character encodings in your text\n",
    "new_test = unicodedata.normalize('NFC', 'test')\n",
    "len(new_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7780d9-de74-4492-934d-b6933cda1338",
   "metadata": {},
   "source": [
    "### Word Frequency counts to identify problematic words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4b3f6c-4d00-4370-81ca-61b57c2947ce",
   "metadata": {},
   "source": [
    "Running a word frequency count can help us start building a custom stopword list. It can also help us identify if there are any variant spellings we want to normalize (using `.replace()` as above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05be050-3218-48fa-af21-309ac95ddf06",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Counting all the words in a text\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "#defining a tokenizing function which will split at and remove whitespace and punctuation\n",
    "#return words and numbers\n",
    "def tokenize(text):\n",
    "    lowercase_text = text.lower()\n",
    "    split_words = re.split('\\W+', lowercase_text)\n",
    "    return split_words\n",
    "\n",
    "#Read in text\n",
    "text = open('kafka_metamorphosis.txt', encoding=\"utf-8\").read()\n",
    "\n",
    "#Use our tokenizing function to tokenize the text\n",
    "all_the_words = tokenize(text)\n",
    "\n",
    "##Count frequencies of all the words\n",
    "all_the_words_count = Counter(all_the_words)\n",
    "all_the_words_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0c8329-a08d-4b1e-b95f-2c46f0d6d5ab",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Counting most frequent words in a text\n",
    "\n",
    "#How many most frequent words do you want to see?\n",
    "number_of_desired_words = 50\n",
    "\n",
    "#Return most frequent words\n",
    "most_frequent_all_the_words_count = all_the_words_count.most_common(number_of_desired_words)\n",
    "most_frequent_all_the_words_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8975cfe2-9c6d-4791-bb4c-2ba60baa65a1",
   "metadata": {},
   "source": [
    "What words in this list are not relevant to your project? You can add them to your custom stopwords list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e88091-048a-4c2a-9cb7-9038d56718da",
   "metadata": {},
   "source": [
    "### Building a custom stopwords list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ae0bb4-5679-4b7f-bf52-41270a39d34f",
   "metadata": {},
   "source": [
    "As we've discussed, you might want to build your own custom stopwords list for your project.\n",
    "\n",
    "You could use an existing stopwords list as your starting point and remove and add words that you want for your project. \n",
    "\n",
    "For example, you could start with the spaCy stopwords list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685cac3b-90d0-4e30-9133-d83aaa3c06e2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Stopwords in spaCy\n",
    "import spacy\n",
    "\n",
    "#Download the language model you're interested in (this is the English pipeline)\n",
    "#For french: fr_core_news_sm\n",
    "#For spanish: es_core_news_sm\n",
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67b8e65-f071-4c94-b89c-f295b4476f13",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Load language model and stopwords list\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "stopwords = nlp.Defaults.stop_words\n",
    "sorted(list(stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca7a567-43e4-4659-bbe4-8a4d99c593fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write out the spacy stopwords list to a txt file\n",
    "with open(\"spacy-stopwords-english.txt\", \"a\") as file_object:\n",
    "    for word in sorted(list(stopwords)): \n",
    "        file_object.write(word + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13addcec-464e-46f2-a960-12539897d666",
   "metadata": {},
   "source": [
    "**Read in your stopwords list, use it in code, and add words to your stopwords list**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d868f43-bc73-4b8c-9bd2-acbe717982dc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Open your txt file and convert to a Python list\n",
    "with open(\"spacy-stopwords-english.txt\", \"r\") as file_object:\n",
    "    custom_stopwords = [s.rstrip('\\n') for s in file_object.readlines()] \n",
    "\n",
    "custom_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b701f959-6465-4fe6-81df-f2a1d08dab41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Append a new work to the list\n",
    "custom_stopwords.append('got')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca2114d-725c-4c0a-a23d-56ecd1a300b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Append multiple new words to the list\n",
    "custom_stopwords += ['gotten', 'mr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3764dfa5-f906-4ee0-9219-ec99efbdc813",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove a word\n",
    "#Find the index of the word you want to remove\n",
    "index = custom_stopwords.index('gotten')\n",
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964ad910-f0f4-4f84-a006-e4be292d27b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#then delete word\n",
    "del custom_stopwords[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf815548-d7be-40f4-a590-ad20e2dc2239",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write out the updated list and sort alphabetically\n",
    "with open(\"custom-stopwords.txt\", \"w\") as file_object:\n",
    "    for word in sorted(custom_stopwords):\n",
    "        file_object.write(word + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c153b81-5510-4f53-a623-fff3c61b635c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if a given word in list (True if in list, False if not in list)\n",
    "'friend' in custom_stopwords "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166e3979-73e4-4101-b868-6e7ba01fc9f9",
   "metadata": {},
   "source": [
    "**Example of using your custom stopword list in code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c3f49f-b513-4129-bedf-6c1e0fef0306",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Open your txt file and covert to a Python list\n",
    "with open(\"custom-stopwords.txt\", \"r\") as file_object:\n",
    "    custom_stopwords = [s.rstrip('\\n') for s in file_object.readlines()] \n",
    "\n",
    "custom_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb2fdc3-f238-43b1-8408-10baa70b087f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "#Defining a tokenizing function\n",
    "def tokenize(text):\n",
    "    lowercase_text = text.lower()\n",
    "    split_words = re.split('\\W+', lowercase_text)\n",
    "    return split_words\n",
    "\n",
    "#Reading in text\n",
    "text = open('kafka_metamorphosis.txt', encoding=\"utf-8\").read()\n",
    "\n",
    "#Tokenizing text\n",
    "all_the_words = tokenize(text)\n",
    "\n",
    "#Filtering only the words not on stopwords list (you use your stopwords list variable here)\n",
    "meaningful_words = [word for word in all_the_words if word not in custom_stopwords]\n",
    "\n",
    "#Counting words\n",
    "meaningful_words_tally = Counter(meaningful_words)\n",
    "\n",
    "#How many frequent words we want to see\n",
    "number_of_desired_words = 50\n",
    "\n",
    "#Return most frequent words\n",
    "most_frequent_meaningful_words = meaningful_words_tally.most_common(number_of_desired_words)\n",
    "most_frequent_meaningful_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35bbc0f4-df27-4a8c-89a4-4ae0d4c3cd5f",
   "metadata": {},
   "source": [
    "****\n",
    "**Read in your stopwords list, use it in code, and add words to your stopwords list**\n",
    "\n",
    "This does the same thing as above but using pandas instead of Python lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94c18ad-d20f-4886-85ac-1c4c24e13e1b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#Read in stopwords list as pandas dataframe and convert to it to a list\n",
    "stopwords_df = pd.read_csv('spacy-stopwords-english.txt', names=['word'])\n",
    "custom_stopwords_list = stopwords_df['word'].to_list()\n",
    "custom_stopwords_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6899c74c-6fdd-428b-9b71-3ac0f72e9727",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding words to the list\n",
    "\n",
    "#Create list of words you want to add\n",
    "new_words = ['got', 'mr']\n",
    "\n",
    "#Create a dataframe of words you want to add\n",
    "new_words_df = pd.DataFrame(new_words, columns=['word'])\n",
    "new_words_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb098141-f1d1-4e66-969e-e8ff94fd2873",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenate/merge the old dataframe with new dataframe with new words in it, and sort it\n",
    "updated_stopwords_df = pd.concat([stopwords_df, new_words_df], ignore_index=True)\n",
    "updated_stopwords_df = updated_stopwords_df.sort_values(by='word')\n",
    "updated_stopwords_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd1f13f-121c-4ca9-b73b-461987133434",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if a word is in the list (False if not in list, True if in list)\n",
    "stopwords_df.word.str.contains('friend').any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6464be-3b8c-4fa7-aea4-edceed86ed39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write out the dataframe to a txt file\n",
    "updated_stopwords_df.to_csv('custom_stopwords.txt', sep=' ', header=None, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9421288c-2c69-46b5-b0cc-05d624f9e29d",
   "metadata": {},
   "source": [
    "**Example of using your custom stopword list in code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca54bdc-e95f-4840-ada1-f822fc16f126",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#Read in stopwords list as pandas dataframe and convert to it to a list\n",
    "stopwords_df = pd.read_csv('custom-stopwords.txt', names=['word'])\n",
    "custom_stopwords_list = stopwords_df['word'].to_list()\n",
    "custom_stopwords_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdcbd94-0e3f-40f8-958e-ac66211e2b80",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "#Defining a tokenizing function\n",
    "def tokenize(text):\n",
    "    lowercase_text = text.lower()\n",
    "    split_words = re.split('\\W+', lowercase_text)\n",
    "    return split_words\n",
    "\n",
    "#Reading in text\n",
    "text = open('kafka_metamorphosis.txt', encoding=\"utf-8\").read()\n",
    "\n",
    "#Tokenizing text\n",
    "all_the_words = tokenize(text)\n",
    "\n",
    "#Filtering only the words not on stopwords list (you use your stopwords list variable here)\n",
    "meaningful_words = [word for word in all_the_words if word not in custom_stopwords_list]\n",
    "\n",
    "#Counting words\n",
    "meaningful_words_tally = Counter(meaningful_words)\n",
    "\n",
    "#How many frequent words we want to see\n",
    "number_of_desired_words = 50\n",
    "\n",
    "#Return most frequent words\n",
    "most_frequent_meaningful_words = meaningful_words_tally.most_common(number_of_desired_words)\n",
    "most_frequent_meaningful_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddfd193-3a5b-499a-9e20-7f1236133992",
   "metadata": {},
   "source": [
    "<a id='section-2'></a>\n",
    "## Step 2: Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3eb8a7-8963-4f27-b7ae-eb0a14ba4d19",
   "metadata": {},
   "source": [
    "### Creating a lemmatized version of your corpus\n",
    "\n",
    "For methods that rely on word counts (e.g. frequency counts, Tf-idf), it's best to use lemmatized text so that a maximum number of words we want counted togther will be counted together. There is evidence that lemmatization is not necessary, maybe even counterproductive for topic modeling.  \n",
    "\n",
    "> \"Stemming has been found to provide little measurable benefits for topic modeling and can sometimes even be harmful (Schofield and Mimno, 2016).\" (Nguyen et al., \"How We Do Things With Words,\" p. 8)\n",
    "\n",
    "\n",
    "It might be good practice to have a lemmatized and unlemmatized version of your corpus so you can experiment with which one produces the most meaningful outputs.\n",
    "\n",
    "Below we create a lemmatized version of the kafka corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d7ca6c-39ba-45e6-93ee-edbfeb6f7f23",
   "metadata": {},
   "source": [
    "**Lemmatizing mutiple files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08ae34e-c35d-4799-85c0-344e1b2f9291",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#This loops over multiple files in a directory\n",
    "#but it might make the kernel crash if it runs out memory\n",
    "#If the kernel crash you might have to lemmatize single files at a time (cf. below)\n",
    "\n",
    "#Lemmatizing using spaCy for English\n",
    "import spacy\n",
    "import glob\n",
    "\n",
    "#Download the language model you're interested in (this is the English pipeline)\n",
    "#For french: fr_core_news_sm\n",
    "#For spanish: es_core_news_sm\n",
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4045ce9a-ead8-4e75-a2de-ecd67875dd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load language model (it needs to match the name above)\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d57a3d-837c-406a-a9df-a4189d5052bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Open your texts and create spaCy document\n",
    "filepath = 'kafka-corpus/'\n",
    "text_files = glob.glob(f'{filepath}/*.txt')\n",
    "\n",
    "#Loop through the files and open as spacy document\n",
    "for file in text_files:\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "        print(file)\n",
    "        document = nlp(text)\n",
    "        \n",
    "    #Lemmatize\n",
    "    outname = file.replace('.txt', '-lemmatized.txt')\n",
    "    with open(outname, 'w', encoding='utf8') as out:   \n",
    "        for token in document:\n",
    "            # Get the lemma for each token\n",
    "            out.write(token.lemma_.lower())\n",
    "            # Insert white space between each token\n",
    "            out.write(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08466d2a-f9cb-4f95-b149-66ae1dab3ff3",
   "metadata": {},
   "source": [
    "**Lemmatizing single files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575f0eef-7c63-4839-b7b5-0735e124e35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatizing single files\n",
    "\n",
    "#Lemmatizing using spaCy for English\n",
    "import spacy\n",
    "#!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac0a0d8-1845-40ad-8f06-c97fe26a7197",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load language model (it needs to match the name above)\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "#Open your text and create spaCy document\n",
    "filepath = 'kafka_metamorphosis.txt'\n",
    "text = open(filepath, encoding='utf-8').read()\n",
    "document = nlp(text)\n",
    "\n",
    "outname = filepath.replace('.txt', '-lemmatized.txt')\n",
    "with open(outname, 'w', encoding='utf8') as out:   \n",
    "    for token in document:\n",
    "        # Get the lemma for each token\n",
    "        out.write(token.lemma_.lower())\n",
    "        # Insert white space between each token\n",
    "        out.write(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac64b71-e856-4cc9-b6a1-be62e55fe38e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#prints the original word in the text, \n",
    "#a dash, then the lemmatized form that was written to the derivative text document\n",
    "#check if there are places where the model consistently makes mistakes\n",
    "#this prints the first 50 tokens - modify the slice next to document for more\n",
    "for token in document[:50]:\n",
    "    print(token.text + ' - ' + token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bece0b21-c6c0-47c7-ab24-0704fb96b815",
   "metadata": {},
   "source": [
    "<a id='section-3'></a>\n",
    "## Step 3: Textual units of analysis: chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f956e2fb-7171-4393-b142-a365d3a59e52",
   "metadata": {},
   "source": [
    "Having texts of widely different lengths might skew the analyses. It's good practice to ensure that the texts are of roughly similar lengths. This might mean joining texts together into larger text blocks if they are very short (e.g. tweets), or splitting longer texts into shorter units.\n",
    "\n",
    "This process can be called chunking. It is also sometimes referred to as segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747cdef0-d0ef-4da6-94c4-9bb185b00719",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loop through and check how long the texts are\n",
    "import glob\n",
    "\n",
    "#Open your texts\n",
    "filepath = 'kafka-corpus/'\n",
    "text_files = glob.glob(f'{filepath}/*.txt')\n",
    "\n",
    "#Loop through the files and print text file name with number of words\n",
    "for file in text_files:\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "        print(file, len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f784fefd-81d8-4492-be53-82eb6ed8cdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split long texts into shorter units\n",
    "#Split into a collection of documents of 3000 words\n",
    "\n",
    "#Loop through the files and print text file name with number of words\n",
    "for file in text_files:\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "        print(file, len(text))\n",
    "        \n",
    "        segment_length = 3000\n",
    "        \n",
    "        nseg = round(len(text) / segment_length)\n",
    "        for i in range(nseg):\n",
    "            segment = text[segment_length*i:segment_length*(i+1)]\n",
    "            outname = file.replace('kafka-corpus/', 'kafka-corpus/kafka-segmented/').replace('.txt', f'-{i}.txt')\n",
    "            with open(outname, 'w', encoding='utf8') as out:\n",
    "                text_chunk = ''.join(segment)\n",
    "                out.write(text_chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4fd731-96f6-4a68-ad17-4991447b1da4",
   "metadata": {},
   "source": [
    "Note that this is a coarse kind of splitting - it can split in the middle of words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577be100-9d02-4351-af8a-0b462ec6dafc",
   "metadata": {},
   "source": [
    "<a id='section-4'></a>\n",
    "## Step 4: Tokenizing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8148938-0529-47c5-bac4-8f6f0b1edbf8",
   "metadata": {},
   "source": [
    "Tokenizing involves splitting the text into units of analysis you're interested in analyzing - most often this is assumed to be \"words\".  \n",
    "\n",
    "We need to do this because, if you look back to the very start of this notebook, unstructured \"raw\" text is just sequences of character encodings, so the analyses will count individual character encodings. We need to restructure our text into the units we want to analyze (i.e. usually \"words\" or \"tokens\").\n",
    "\n",
    "Tokenization works by defining markers at which you split the string. Different tokenizing procedures might use different markers.\n",
    "\n",
    "Most methods have built-in tokenizing functions (e.g. cf. TF-IDF notebook about scikit-learn's built-in tokenizing procedure that you can override with your own). \n",
    "\n",
    "You can use the built-in tokenizing procedures, or you can define and use your own. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2683085d-c8e8-4350-966e-45a30e4d5e84",
   "metadata": {},
   "source": [
    "Here are examples of different tokenizing procedures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a15b78-54ef-4ffe-9e86-d3cc090445f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the .split() method in Python uses whitespace as default\n",
    "text = \"I'd say, they're happy it's mother's day.\"\n",
    "text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ac220c-b016-46c0-a3c5-6b0f9511641d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#you can also pass different markers to .split() to define where you want to split your text\n",
    "#this uses regular expression to split at any one character or more that is NOT a word\n",
    "import re\n",
    "text = \"I'd say, they're happy it's mother's day.\"\n",
    "tokens = re.split('\\W+', text)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939bc10d-99ab-4073-bb30-7bb8aa20617c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Built-in tokenizing procedure in NLTK\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "text = \"I'd say, they're happy it's mother's day.\"\n",
    "tokens = nltk.word_tokenize(text)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ade03e-0dc9-4c67-9ab1-1d3f49df739f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Built-in tokenizing procedure in spaCy in Spanish\n",
    "#Download model\n",
    "import spacy\n",
    "!python -m spacy download es_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c95da3b-f990-4ce2-b782-752ece3666b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load language model\n",
    "nlp = spacy.load('es_core_news_sm')\n",
    "#Create spaCy process document\n",
    "text = 'Yo dir칤a, que est치n felices de que sea el d칤a de la madre.'\n",
    "document = nlp(text)\n",
    "\n",
    "tokens = [token.text for token in document]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ac5e6f-b51b-4ca2-ab83-a5d71bb97f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Built-in tokenizing procedure in spaCy in French\n",
    "#Download model\n",
    "import spacy\n",
    "!python -m spacy download fr_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9777627d-562b-4dc2-b178-b8f375f7ac3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load language model\n",
    "nlp = spacy.load('fr_core_news_sm')\n",
    "#Create spaCy process document\n",
    "text = 'On dirait qu\\'ils sont heureux que ce soit la f칡te des m칟res.'\n",
    "document = nlp(text)\n",
    "\n",
    "tokens = [token.text for token in document]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f4ae81-adcd-4553-9dd1-1a28daaccdf1",
   "metadata": {},
   "source": [
    "**Defining your own tokenizing functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332aa39f-6a47-4186-a868-c0814b3dc763",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only words, no numbers\n",
    "#Define a function to lowcase, split at and remove anything not a \"word\" character\n",
    "#(i.e. a letter or digit or underbar)\n",
    "#So it will split at and remove whitspace and punctuation\n",
    "#Then keep only alphabetic characters (i.e. remove numbers) with .isalpha()\n",
    "\n",
    "def tokenize(text):\n",
    "    lowercase_text = text.lower()\n",
    "    split_words = re.split('\\W+', lowercase_text)\n",
    "    no_numbers = [word for word in split_words if word.isalpha()]\n",
    "    return no_numbers\n",
    "\n",
    "text_example = \"I'd say, they're happy it's mother's day. 1988!\"\n",
    "tokenized_text_example = tokenize(text_example)\n",
    "tokenized_text_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cde16b-7542-4a16-8a3c-64e80b0f21e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Words and numbers\n",
    "#Define a function to lowcase, split at and remove anything not a \"word\" character\n",
    "#(i.e. a letter or digit or underbar)\n",
    "#So it will split at and remove whitspace and punctuation\n",
    "\n",
    "def tokenize(text):\n",
    "    lowercase_text = text.lower()\n",
    "    split_words = re.split('\\W+', lowercase_text)\n",
    "    return split_words\n",
    "\n",
    "text_example = \"I'd say, they're happy it's mother's day. 1988!\"\n",
    "tokenized_text_example = tokenize(text_example)\n",
    "tokenized_text_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd0dccc-45d7-4b33-8524-589557e419f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
